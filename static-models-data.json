{
  "models": [
    {
      "id": "cmewtbrxp0002lcikxx1jjnm8",
      "name": "claude-3-haiku-20240307",
      "displayName": "Claude 3 Haiku",
      "provider": "Anthropic",
      "description": "Fast and efficient for simple tasks",
      "contextWindow": 200000,
      "pricingInput": 0.00025,
      "pricingOutput": 0.00125,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"summarization\",\"basic_coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.118514052922207,
      "successRate": 0.9613058384068245,
      "passRate": 0.757640379805798,
      "totalRatings": 0,
      "totalBenchmarks": 23,
      "aiderBenchmark": 0.8498021120059518,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewu7ttc0034lcdodzlhxuow",
          "metricName": "success_rate",
          "metricValue": 0.9613058384068245,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T12:57:14.161Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7tv4003clcdopnc1gvf6",
          "metricName": "pass_rate",
          "metricValue": 0.757640379805798,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T12:57:14.224Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7u9d0052lcdowat2mf1x",
          "metricName": "success_rate",
          "metricValue": 0.7041684504719742,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T12:57:14.737Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewu7u8u0050lcdotoj20v04",
          "metricName": "avg_response_time",
          "metricValue": 2.118514052922207,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T12:57:14.718Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewu7twt003klcdo3uj58o3c",
          "metricName": "pass_rate",
          "metricValue": 0.7038773569033735,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T12:57:14.285Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gswl049slcy4jy7e0t1d",
          "metricName": "elo_rating",
          "metricValue": 1217.164881780289,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.198Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7tyk003slcdooww71xku",
          "metricName": "accuracy",
          "metricValue": 0.9233348365509606,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T12:57:14.348Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewu7u070040lcdoz2zrrulp",
          "metricName": "accuracy",
          "metricValue": 0.7645510974007774,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T12:57:14.408Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewu7u240048lcdoyz8yvozc",
          "metricName": "accuracy",
          "metricValue": 0.649944552371877,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T12:57:14.477Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewu7u3x004glcdolw1cidtc",
          "metricName": "accuracy",
          "metricValue": 0.8778374127841009,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T12:57:14.541Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewu7u5y004olcdohkx05lqo",
          "metricName": "overall_score",
          "metricValue": 7.140524188140192,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T12:57:14.614Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewu7ubv005clcdoxnwkm9gu",
          "metricName": "average_score",
          "metricValue": 72.58982936100317,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T12:57:14.827Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewu7udw005klcdoyvqq23yj",
          "metricName": "win_rate",
          "metricValue": 0.5330099782310345,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T12:57:14.900Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewx9b4c000ilcig0tva0lgo",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.8498021120059518,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:22:22.092Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmewwqt6w000clcicyh1iwgiu",
          "metricName": "pass_rate",
          "metricValue": 0.7748508898196748,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T14:07:59.048Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmewwqt6l000alcich1pjpfws",
          "metricName": "avg_response_time",
          "metricValue": 4.064851283406819,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T14:07:59.038Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmex13ius0383lcy4r224ogab",
          "metricName": "accuracy",
          "metricValue": 0.7833688516771296,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.645Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jeq03bdlcy4t9xsu8ct",
          "metricName": "success_rate",
          "metricValue": 0.4048257274089251,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.363Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13jy703enlcy4mv0rxibz",
          "metricName": "overall_score",
          "metricValue": 63.86139749320521,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.063Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kht03hxlcy45v4ybn3a",
          "metricName": "accuracy",
          "metricValue": 0.605612298213893,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.770Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l1i03l7lcy4v5xxtsq6",
          "metricName": "truth_score",
          "metricValue": 0.5455409485296392,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.478Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13llo03ohlcy46idw7s4y",
          "metricName": "accuracy",
          "metricValue": 0.3786697714654407,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.203Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtdq04d2lcy4zw8vh0ce",
          "metricName": "percentile_rank",
          "metricValue": 35.69016374398949,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.814Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:32:18.733Z"
    },
    {
      "id": "cmewubd2u008zlcdoxqw5oeon",
      "name": "claude-3-opus-20240229",
      "displayName": "Claude 3 Opus",
      "provider": "Anthropic",
      "description": "Most capable model for highly complex tasks",
      "contextWindow": 200000,
      "pricingInput": 0.015,
      "pricingOutput": 0.075,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"reasoning\",\"research\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.532942083363211,
      "successRate": 0.8599685728786297,
      "passRate": 0.9901342092341237,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn4500bilcdoxyvktzku",
          "metricName": "success_rate",
          "metricValue": 0.8599685728786297,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.101Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnfl00d8lcdoxakrdkmd",
          "metricName": "pass_rate",
          "metricValue": 0.9901342092341237,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.513Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubpxr00pmlcdoxaka4z8j",
          "metricName": "success_rate",
          "metricValue": 0.8186626502139988,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.759Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubpxe00pklcdo8lspk9hj",
          "metricName": "avg_response_time",
          "metricValue": 2.532942083363211,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.746Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnui00eylcdogky2hgnd",
          "metricName": "pass_rate",
          "metricValue": 0.7120274920009522,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.051Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsxk049ylcy4m95xgj2e",
          "metricName": "elo_rating",
          "metricValue": 972.1934522768977,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.233Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubo7a00golcdokleb9nh1",
          "metricName": "accuracy",
          "metricValue": 0.9681499017132367,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.510Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubojf00ielcdot53bvkfp",
          "metricName": "accuracy",
          "metricValue": 0.9350083340630098,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:13.947Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubovm00k4lcdo1tc96w4f",
          "metricName": "accuracy",
          "metricValue": 0.645651558069267,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.387Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp7s00lulcdoz3p5971l",
          "metricName": "accuracy",
          "metricValue": 0.969317739926582,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.825Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpjh00nklcdoxiilov3j",
          "metricName": "overall_score",
          "metricValue": 9.79207688428055,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.245Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqky00sqlcdof5hgz391",
          "metricName": "average_score",
          "metricValue": 66.58943948382601,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.594Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubqxl00uglcdo2qvotx0g",
          "metricName": "win_rate",
          "metricValue": 0.5901056804481479,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.049Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ivs0389lcy4btvf3pml",
          "metricName": "accuracy",
          "metricValue": 0.5141577640710204,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.680Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jfr03bjlcy4q6swl1t4",
          "metricName": "success_rate",
          "metricValue": 0.5669615840276692,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.399Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13jz603etlcy4ng9b870n",
          "metricName": "overall_score",
          "metricValue": 53.47544870236968,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.098Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kiu03i3lcy4hhhegq01",
          "metricName": "accuracy",
          "metricValue": 0.4477580880732683,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.807Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l2h03ldlcy4ykmitoo5",
          "metricName": "truth_score",
          "metricValue": 0.6872134157110823,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.513Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lmo03onlcy4wbqhx2v5",
          "metricName": "accuracy",
          "metricValue": 0.5104599458695493,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.240Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtep04d8lcy4ocp3ixl9",
          "metricName": "percentile_rank",
          "metricValue": 52.33027784065321,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.849Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.094Z"
    },
    {
      "id": "cmewubd340090lcdo4uwipt4j",
      "name": "claude-3-sonnet-20240229",
      "displayName": "Claude 3 Sonnet",
      "provider": "Anthropic",
      "description": "Balanced model for general tasks",
      "contextWindow": 200000,
      "pricingInput": 0.003,
      "pricingOutput": 0.015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.181931073057695,
      "successRate": 0.7663646717581467,
      "passRate": 0.6962987672135676,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn4i00bklcdohvao5i8f",
          "metricName": "success_rate",
          "metricValue": 0.7663646717581467,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.114Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubng600dalcdorg9v8qtq",
          "metricName": "pass_rate",
          "metricValue": 0.6962987672135676,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.534Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubpyh00pqlcdom0zqdcy1",
          "metricName": "success_rate",
          "metricValue": 0.8067362808185036,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.785Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubpy300polcdoba75o0ai",
          "metricName": "avg_response_time",
          "metricValue": 2.181931073057695,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.771Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnuv00f0lcdo5e2ki3t3",
          "metricName": "pass_rate",
          "metricValue": 0.7337296021492844,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.063Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsxt04a0lcy4v1uwvm2t",
          "metricName": "elo_rating",
          "metricValue": 1698.279123151242,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.242Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubo7p00gqlcdo9c76xh04",
          "metricName": "accuracy",
          "metricValue": 0.8209958889724908,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.525Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubojt00iglcdozugkzpth",
          "metricName": "accuracy",
          "metricValue": 0.8707403093702293,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:13.961Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubovy00k6lcdoxzzkc6we",
          "metricName": "accuracy",
          "metricValue": 0.8627754012575939,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.399Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp8400lwlcdout64bfky",
          "metricName": "accuracy",
          "metricValue": 0.8537702988272903,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.836Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpjv00nmlcdob5q2vwi8",
          "metricName": "overall_score",
          "metricValue": 9.618447959915525,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.260Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqld00sslcdo1i5sgn1c",
          "metricName": "average_score",
          "metricValue": 61.6910664830958,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.609Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubqxz00uilcdoe0moamtk",
          "metricName": "win_rate",
          "metricValue": 0.6432826659626816,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.063Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13iw4038blcy4zgggsyu2",
          "metricName": "accuracy",
          "metricValue": 0.7925197100085023,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.692Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jg403bllcy4s7s0zeg0",
          "metricName": "success_rate",
          "metricValue": 0.3720673606061307,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.412Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13jzh03evlcy44upr96dr",
          "metricName": "overall_score",
          "metricValue": 64.55972339464009,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.110Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kj503i5lcy4aj2p95n7",
          "metricName": "accuracy",
          "metricValue": 0.385763607755135,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.818Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l2s03lflcy442hpi0dr",
          "metricName": "truth_score",
          "metricValue": 0.8154494962117831,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.524Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13ln003oplcy44bkxpnmg",
          "metricName": "accuracy",
          "metricValue": 0.4548676772322585,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.252Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtez04dalcy4ybw1n97p",
          "metricName": "percentile_rank",
          "metricValue": 15.59082433017029,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.859Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.105Z"
    },
    {
      "id": "cmewubd2l008ylcdoccig6hnw",
      "name": "claude-3-5-haiku-20241022",
      "displayName": "Claude 3.5 Haiku",
      "provider": "Anthropic",
      "description": "Fast and efficient model for general tasks",
      "contextWindow": 200000,
      "pricingInput": 0.0008,
      "pricingOutput": 0.004,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"summarization\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.326974116228566,
      "successRate": 0.8061581738856342,
      "passRate": 0.6989386382549851,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn3s00bglcdo6uwzuj77",
          "metricName": "success_rate",
          "metricValue": 0.8061581738856342,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.089Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnez00d6lcdoaqm6x9yn",
          "metricName": "pass_rate",
          "metricValue": 0.6989386382549851,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.492Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubpx000pilcdoaddzbqmd",
          "metricName": "success_rate",
          "metricValue": 0.7697565363689116,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.732Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubpwo00pglcdo8wzvponp",
          "metricName": "avg_response_time",
          "metricValue": 2.326974116228566,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.720Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnu400ewlcdoxb3p7rxo",
          "metricName": "pass_rate",
          "metricValue": 0.7663247489909502,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.036Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsx9049wlcy4n3dq9ycy",
          "metricName": "elo_rating",
          "metricValue": 1442.186068934476,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.221Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubo6u00gmlcdoejsg8mq8",
          "metricName": "accuracy",
          "metricValue": 0.8947474392683337,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.495Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuboj100iclcdo740bakjo",
          "metricName": "accuracy",
          "metricValue": 0.6476598132467141,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:13.933Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubovb00k2lcdoivck8dmz",
          "metricName": "accuracy",
          "metricValue": 0.7515253754987051,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.375Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp7f00lslcdo01yufvul",
          "metricName": "accuracy",
          "metricValue": 0.985992447190326,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.811Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpj300nilcdor411ezmj",
          "metricName": "overall_score",
          "metricValue": 8.019728967680376,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.231Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqkm00solcdo17ew9oes",
          "metricName": "average_score",
          "metricValue": 79.15116761899984,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.582Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubqx500uelcdogthjr25w",
          "metricName": "win_rate",
          "metricValue": 0.7147329085604754,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.033Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ivf0387lcy4bgntfvrq",
          "metricName": "accuracy",
          "metricValue": 0.4635991677446675,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.667Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jfe03bhlcy43kcea307",
          "metricName": "success_rate",
          "metricValue": 0.3188123592032542,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.387Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13jyu03erlcy42wpbd7fa",
          "metricName": "overall_score",
          "metricValue": 74.5961194839851,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.087Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kii03i1lcy4uhraiizp",
          "metricName": "accuracy",
          "metricValue": 0.6536280612756329,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.795Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l2703lblcy4t2207xia",
          "metricName": "truth_score",
          "metricValue": 0.875223807618787,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.503Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lmc03ollcy4hkicvd42",
          "metricName": "accuracy",
          "metricValue": 0.3762544021180405,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.228Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gted04d6lcy4pydolnbb",
          "metricName": "percentile_rank",
          "metricValue": 19.4182989179369,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.837Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.085Z"
    },
    {
      "id": "cmewtbrws0000lcik751r1yol",
      "name": "claude-3-5-sonnet-20241022",
      "displayName": "Claude 3.5 Sonnet",
      "provider": "Anthropic",
      "description": "Most intelligent model for complex reasoning and coding tasks",
      "contextWindow": 200000,
      "pricingInput": 0.003,
      "pricingOutput": 0.015,
      "isFree": false,
      "isRecommended": true,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"code_review\",\"debugging\",\"documentation\",\"analysis\",\"reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.167310854413513,
      "successRate": 0.7311711192088557,
      "passRate": 0.8775602699250639,
      "totalRatings": 0,
      "totalBenchmarks": 24,
      "aiderBenchmark": 0.8629719322533322,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewu7tsi0030lcdoyrw9d6gm",
          "metricName": "success_rate",
          "metricValue": 0.7311711192088557,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T12:57:14.130Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7tu80038lcdo36hup9bo",
          "metricName": "pass_rate",
          "metricValue": 0.8775602699250639,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T12:57:14.192Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7u7f004ulcdoeu15dl9l",
          "metricName": "success_rate",
          "metricValue": 0.9209815294985568,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T12:57:14.667Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewu7u6z004slcdo2tsnhiu5",
          "metricName": "avg_response_time",
          "metricValue": 2.167310854413513,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T12:57:14.651Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewu7tw0003glcdo1y9n6pe9",
          "metricName": "pass_rate",
          "metricValue": 0.6396345601808087,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T12:57:14.256Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsvz049olcy4reviwzhi",
          "metricName": "elo_rating",
          "metricValue": 1368.315906178426,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.175Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7txo003olcdomdmj5gac",
          "metricName": "accuracy",
          "metricValue": 0.8549360045274459,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T12:57:14.316Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewu7tze003wlcdofyqdapvy",
          "metricName": "accuracy",
          "metricValue": 0.7398310527046171,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T12:57:14.378Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewu7u140044lcdombokegmw",
          "metricName": "accuracy",
          "metricValue": 0.9610976265719667,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T12:57:14.441Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewu7u34004clcdorg32t0wy",
          "metricName": "accuracy",
          "metricValue": 0.6499224153621347,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T12:57:14.512Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewu7u4y004klcdoptdzccm6",
          "metricName": "overall_score",
          "metricValue": 6.022944423369993,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T12:57:14.579Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewu7uaw0058lcdoo1k3twlo",
          "metricName": "average_score",
          "metricValue": 68.02413490544033,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T12:57:14.792Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewu7ucy005glcdod96mzba6",
          "metricName": "win_rate",
          "metricValue": 0.6460868966021006,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T12:57:14.866Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewx9b330006lcigbyvo3t9d",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.8629719322533322,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:22:22.047Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmewwqt5w0004lcic6i69q7jr",
          "metricName": "pass_rate",
          "metricValue": 0.8998683447810777,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T14:07:59.012Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmewwqt5n0002lcicmb821y6r",
          "metricName": "avg_response_time",
          "metricValue": 3.252262221324966,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T14:07:59.004Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmex13iu6037zlcy4jl58m39n",
          "metricName": "accuracy",
          "metricValue": 0.4240084035051669,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.623Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13je203b9lcy4539apdxj",
          "metricName": "success_rate",
          "metricValue": 0.4559121586271029,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.338Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13jxk03ejlcy4mz5n7j4w",
          "metricName": "overall_score",
          "metricValue": 52.43770215636866,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.040Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kh803htlcy4lqjmbwrz",
          "metricName": "accuracy",
          "metricValue": 0.6719660175606943,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.748Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l0v03l3lcy4pl135emd",
          "metricName": "truth_score",
          "metricValue": 0.8965448390905408,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.456Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lky03odlcy4fphnw1ff",
          "metricName": "accuracy",
          "metricValue": 0.4391021471063384,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.178Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e10m0002lc1oyge7ch4d",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.89,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.742Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtd304cylcy4rhg8orns",
          "metricName": "percentile_rank",
          "metricValue": 44.62143128649647,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.791Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:32:18.697Z"
    },
    {
      "id": "cmewubd7q009ilcdoqyk1kjm8",
      "name": "codestral-22b",
      "displayName": "Codestral 22B",
      "provider": "Mistral",
      "description": "Powerful coding model",
      "contextWindow": 32000,
      "pricingInput": 0.00018,
      "pricingOutput": 0.00018,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"code_review\",\"debugging\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.314522951968724,
      "successRate": 0.8386394240201205,
      "passRate": 0.8443761567596187,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn9v00cglcdouy18eqxs",
          "metricName": "success_rate",
          "metricValue": 0.8386394240201205,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.308Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnos00e6lcdovpokjdoa",
          "metricName": "pass_rate",
          "metricValue": 0.8443761567596187,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.844Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqbv00rilcdo9aoptk0p",
          "metricName": "success_rate",
          "metricValue": 0.8001333959282732,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.267Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqbh00rglcdoch81jcl0",
          "metricName": "avg_response_time",
          "metricValue": 3.314522951968724,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.253Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo1h00fwlcdo683dokw6",
          "metricName": "pass_rate",
          "metricValue": 0.8673909678460923,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.301Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt2c04awlcy42qafslbe",
          "metricName": "elo_rating",
          "metricValue": 993.264048889031,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.405Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewuboe700hmlcdodwjh8nro",
          "metricName": "accuracy",
          "metricValue": 0.9312890549269441,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.760Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuboq700jclcdonf7195oi",
          "metricName": "accuracy",
          "metricValue": 0.7673930159910591,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.191Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp2a00l2lcdoxem3koq3",
          "metricName": "accuracy",
          "metricValue": 0.9020338533119227,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.626Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpds00mslcdolvgsrs5e",
          "metricName": "accuracy",
          "metricValue": 0.8248741999427509,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.040Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpqd00oilcdo452c62r6",
          "metricName": "overall_score",
          "metricValue": 7.374193755572922,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.493Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqrr00tolcdobgqkb78k",
          "metricName": "average_score",
          "metricValue": 61.51107600894568,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.839Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr4r00velcdodi40wm35",
          "metricName": "win_rate",
          "metricValue": 0.446462302917004,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.308Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j1v0397lcy4ndyazte8",
          "metricName": "accuracy",
          "metricValue": 0.7686941974041851,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.899Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jli03chlcy4ujr34jak",
          "metricName": "success_rate",
          "metricValue": 0.3797083041955265,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.607Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k4q03frlcy4q7ck4008",
          "metricName": "overall_score",
          "metricValue": 58.29848738870214,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.299Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kod03j1lcy46cl1uxha",
          "metricName": "accuracy",
          "metricValue": 0.2160300723332021,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.006Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l8a03mblcy45yqlea70",
          "metricName": "truth_score",
          "metricValue": 0.6013842378632481,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.722Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13ls003pllcy4s3uqtbow",
          "metricName": "accuracy",
          "metricValue": 0.5397563002045133,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.432Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtjj04e6lcy4wt1139sn",
          "metricName": "percentile_rank",
          "metricValue": 10.14403807646324,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.024Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.270Z"
    },
    {
      "id": "cmewubd7i009hlcdo46mudgpu",
      "name": "codestral-mamba",
      "displayName": "Codestral Mamba",
      "provider": "Mistral",
      "description": "Specialized coding model with large context",
      "contextWindow": 256000,
      "pricingInput": 0.00018,
      "pricingOutput": 0.00018,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"code_review\",\"debugging\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.32472058185542,
      "successRate": 0.9694947162672873,
      "passRate": 0.7677996425994291,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn9h00celcdoz4pqhy3q",
          "metricName": "success_rate",
          "metricValue": 0.9694947162672873,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.293Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubno900e4lcdon699hk91",
          "metricName": "pass_rate",
          "metricValue": 0.7677996425994291,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.826Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqb100relcdocokvkhpk",
          "metricName": "success_rate",
          "metricValue": 0.9421104045392741,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.237Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqam00rclcdo5y01csds",
          "metricName": "avg_response_time",
          "metricValue": 3.32472058185542,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.222Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo1000fulcdojq33z2fb",
          "metricName": "pass_rate",
          "metricValue": 0.5399287129179093,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.285Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt2204aulcy415n4p4w2",
          "metricName": "elo_rating",
          "metricValue": 1522.454246768065,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.394Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubodt00hklcdo3cgtats6",
          "metricName": "accuracy",
          "metricValue": 0.7051973535953849,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.745Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubopu00jalcdo3h3ye3gr",
          "metricName": "accuracy",
          "metricValue": 0.8450823008335078,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.178Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp1v00l0lcdo9u5d5tog",
          "metricName": "accuracy",
          "metricValue": 0.9157798277622368,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.611Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpdf00mqlcdo7rruqy7g",
          "metricName": "accuracy",
          "metricValue": 0.9419829432913941,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.028Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpq100oglcdoj7twkosv",
          "metricName": "overall_score",
          "metricValue": 7.293770743465421,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.481Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqrb00tmlcdoeezhticy",
          "metricName": "average_score",
          "metricValue": 74.41504276637248,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.823Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr4c00vclcdojvpaec5n",
          "metricName": "win_rate",
          "metricValue": 0.6899392359731698,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.292Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j1j0395lcy4frbecvkf",
          "metricName": "accuracy",
          "metricValue": 0.7063511965176366,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.887Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jl603cflcy4zgdwr84j",
          "metricName": "success_rate",
          "metricValue": 0.4721145749166967,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.594Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k4e03fplcy4ggh6xz8e",
          "metricName": "overall_score",
          "metricValue": 69.33540527095427,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.286Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ko203izlcy42zs0d7qa",
          "metricName": "accuracy",
          "metricValue": 0.6289040220983824,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.994Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l7x03m9lcy4jqc363jq",
          "metricName": "truth_score",
          "metricValue": 0.8463680818243098,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.710Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lrn03pjlcy4hkqv5emz",
          "metricName": "accuracy",
          "metricValue": 0.5001553579570884,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.420Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtj904e4lcy49uo0x40w",
          "metricName": "percentile_rank",
          "metricValue": 16.93769726453494,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.013Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.262Z"
    },
    {
      "id": "cmewubd8r009mlcdove6ex0m3",
      "name": "command-r",
      "displayName": "Command R",
      "provider": "Cohere",
      "description": "Balanced command model from Cohere",
      "contextWindow": 128000,
      "pricingInput": 0.0005,
      "pricingOutput": 0.0015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.728450691738532,
      "successRate": 0.7041796322349032,
      "passRate": 0.8869385935281542,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubnbm00colcdofkd3mf8w",
          "metricName": "success_rate",
          "metricValue": 0.7041796322349032,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.371Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnqo00eelcdolze5vsxe",
          "metricName": "pass_rate",
          "metricValue": 0.8869385935281542,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.912Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqf800rylcdo16vbx8dv",
          "metricName": "success_rate",
          "metricValue": 0.8104477101757218,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.389Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqeu00rwlcdode77usw1",
          "metricName": "avg_response_time",
          "metricValue": 3.728450691738532,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.374Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo3000g4lcdogfj7cxwa",
          "metricName": "pass_rate",
          "metricValue": 0.7667319999359838,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.356Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt3j04b4lcy4dka5wm1o",
          "metricName": "elo_rating",
          "metricValue": 1129.022247112818,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.448Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubofj00hulcdo7owh3uba",
          "metricName": "accuracy",
          "metricValue": 0.7105579533934252,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.807Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuborn00jklcdozcbzr326",
          "metricName": "accuracy",
          "metricValue": 0.6833329104346385,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.243Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp3t00lalcdo2i03rz55",
          "metricName": "accuracy",
          "metricValue": 0.7313743696772594,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.681Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpf900n0lcdodbk5w3ww",
          "metricName": "accuracy",
          "metricValue": 0.8985163939152092,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.093Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubprv00oqlcdoq2oj95we",
          "metricName": "overall_score",
          "metricValue": 7.529102125301698,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.548Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqth00twlcdomsmme2rd",
          "metricName": "average_score",
          "metricValue": 79.94036191436179,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.902Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr6c00vmlcdozzuyd435",
          "metricName": "win_rate",
          "metricValue": 0.748909413731171,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.364Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j38039flcy4pneakzc7",
          "metricName": "accuracy",
          "metricValue": 0.7384242592850001,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.948Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jms03cplcy45t05sdfo",
          "metricName": "success_rate",
          "metricValue": 0.4889645881849429,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.653Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k6b03fzlcy4lxpr4e2m",
          "metricName": "overall_score",
          "metricValue": 51.69980082546674,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.355Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kpp03j9lcy4jcn8utwk",
          "metricName": "accuracy",
          "metricValue": 0.2517217305936626,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.053Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l9l03mjlcy4wjotjemr",
          "metricName": "truth_score",
          "metricValue": 0.6206362489095113,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.769Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lta03ptlcy4bc1vroxw",
          "metricName": "accuracy",
          "metricValue": 0.474125992474917,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.478Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtkn04eelcy4tt88283t",
          "metricName": "percentile_rank",
          "metricValue": 17.87860489854909,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.063Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.307Z"
    },
    {
      "id": "cmewubd8h009llcdo1ufabpc1",
      "name": "command-r-plus",
      "displayName": "Command R+",
      "provider": "Cohere",
      "description": "Advanced command model from Cohere",
      "contextWindow": 128000,
      "pricingInput": 0.0015,
      "pricingOutput": 0.0045,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.985658887260548,
      "successRate": 0.8401439483879963,
      "passRate": 0.9894125349197167,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubnbc00cmlcdoaryhpjmy",
          "metricName": "success_rate",
          "metricValue": 0.8401439483879963,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.360Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnq800eclcdok4xqn38j",
          "metricName": "pass_rate",
          "metricValue": 0.9894125349197167,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.896Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqeg00rulcdopk2pvjk6",
          "metricName": "success_rate",
          "metricValue": 0.7977218612933679,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.360Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqe100rslcdonut1ttdq",
          "metricName": "avg_response_time",
          "metricValue": 3.985658887260548,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.345Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo2m00g2lcdole2qqzit",
          "metricName": "pass_rate",
          "metricValue": 0.6244103136088737,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.343Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt3504b2lcy4fj7aiyrj",
          "metricName": "elo_rating",
          "metricValue": 919.1859028529876,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.434Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubof600hslcdoj7m63pr0",
          "metricName": "accuracy",
          "metricValue": 0.9992327930704638,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.795Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubora00jilcdog10g17dw",
          "metricName": "accuracy",
          "metricValue": 0.8089172714418132,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.230Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp3f00l8lcdo05b7ylhh",
          "metricName": "accuracy",
          "metricValue": 0.6008893730683404,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.667Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpeu00mylcdoc9vhp3cz",
          "metricName": "accuracy",
          "metricValue": 0.6237725835316832,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.078Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubprj00oolcdo06wedvsm",
          "metricName": "overall_score",
          "metricValue": 9.812222375429318,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.535Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqt300tulcdotgafsqhv",
          "metricName": "average_score",
          "metricValue": 75.08389600260594,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.887Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr5y00vklcdouzlmn1qo",
          "metricName": "win_rate",
          "metricValue": 0.7561881772587253,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.350Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j2v039dlcy40kbj1ngu",
          "metricName": "accuracy",
          "metricValue": 0.552583968836186,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.935Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jmi03cnlcy4rx5qsmpl",
          "metricName": "success_rate",
          "metricValue": 0.3323530332619353,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.642Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k5w03fxlcy4oa9epxbf",
          "metricName": "overall_score",
          "metricValue": 75.60585944947272,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.341Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kpd03j7lcy4bgxgqhwc",
          "metricName": "accuracy",
          "metricValue": 0.3925003006815481,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.041Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l9903mhlcy4ulnzwd23",
          "metricName": "truth_score",
          "metricValue": 0.861991638765147,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.758Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lsy03prlcy4aygyeal1",
          "metricName": "accuracy",
          "metricValue": 0.4047637211451479,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.466Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtke04eclcy43020tow7",
          "metricName": "percentile_rank",
          "metricValue": 23.72959213402081,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.054Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.297Z"
    },
    {
      "id": "cmewtbrxy0003lcik62iswfdh",
      "name": "cursor-small",
      "displayName": "Cursor Small (Free)",
      "provider": "Cursor",
      "description": "Fast and efficient free tier model",
      "contextWindow": 32000,
      "pricingInput": 0,
      "pricingOutput": 0,
      "isFree": true,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"basic_coding\",\"summarization\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.624204682309561,
      "successRate": 0.7060711197477614,
      "passRate": 0.7228247321075024,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewu7ttr0036lcdo1vb76jh5",
          "metricName": "success_rate",
          "metricValue": 0.7060711197477614,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T12:57:14.175Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7tvj003elcdor17q5qci",
          "metricName": "pass_rate",
          "metricValue": 0.7228247321075024,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T12:57:14.239Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7uad0056lcdoccxtvbk9",
          "metricName": "success_rate",
          "metricValue": 0.7277862866321385,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T12:57:14.773Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewu7u9v0054lcdode9f01po",
          "metricName": "avg_response_time",
          "metricValue": 2.624204682309561,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T12:57:14.756Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewu7tx7003mlcdon74w4lsj",
          "metricName": "pass_rate",
          "metricValue": 0.662204461735088,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T12:57:14.299Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gswx049ulcy45xna9z9p",
          "metricName": "elo_rating",
          "metricValue": 1327.343668836656,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.209Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7tyz003ulcdo6b88z14l",
          "metricName": "accuracy",
          "metricValue": 0.8871132178329767,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T12:57:14.363Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewu7u0o0042lcdo1tkzzmd6",
          "metricName": "accuracy",
          "metricValue": 0.6752184319125663,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T12:57:14.424Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewu7u2l004alcdoqx7o2uj9",
          "metricName": "accuracy",
          "metricValue": 0.6827868744729808,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T12:57:14.493Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewu7u4c004ilcdo84dlbge3",
          "metricName": "accuracy",
          "metricValue": 0.638501783409682,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T12:57:14.556Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewu7u6e004qlcdoclk2d2w3",
          "metricName": "overall_score",
          "metricValue": 9.051723490818961,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T12:57:14.631Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewu7uce005elcdo5t1fgpwa",
          "metricName": "average_score",
          "metricValue": 69.25511795086584,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T12:57:14.846Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewu7uec005mlcdobrk1m35k",
          "metricName": "win_rate",
          "metricValue": 0.7950513510581869,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T12:57:14.916Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13iv40385lcy409rdf91z",
          "metricName": "accuracy",
          "metricValue": 0.4330580290126432,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.656Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jf203bflcy4668othtu",
          "metricName": "success_rate",
          "metricValue": 0.4212913513174014,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.374Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13jyi03eplcy4gyuf3703",
          "metricName": "overall_score",
          "metricValue": 72.06077713123216,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.075Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ki603hzlcy42uuhi8n5",
          "metricName": "accuracy",
          "metricValue": 0.5233248875008587,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.783Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l1v03l9lcy49tox28nb",
          "metricName": "truth_score",
          "metricValue": 0.8525078818452899,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.491Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lm003ojlcy4cbxw2wbw",
          "metricName": "accuracy",
          "metricValue": 0.5889450410625858,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.216Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gte104d4lcy4tnrhvmsn",
          "metricName": "percentile_rank",
          "metricValue": 58.46127479967467,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.825Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:32:18.742Z"
    },
    {
      "id": "cmewubd89009klcdo1lq4egr4",
      "name": "deepseek-chat-67b",
      "displayName": "DeepSeek Chat 67B",
      "provider": "DeepSeek",
      "description": "Large conversational model from DeepSeek",
      "contextWindow": 32768,
      "pricingInput": 0.00014,
      "pricingOutput": 0.00028,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.306185715994168,
      "successRate": 0.8921072119577415,
      "passRate": 0.9466062722173654,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubnb100cklcdotwewnu3b",
          "metricName": "success_rate",
          "metricValue": 0.8921072119577415,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.350Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnps00ealcdo3hs4yc00",
          "metricName": "pass_rate",
          "metricValue": 0.9466062722173654,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.881Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqdl00rqlcdowvqp6spd",
          "metricName": "success_rate",
          "metricValue": 0.803764261336072,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.330Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqd700rolcdoc9arvgfs",
          "metricName": "avg_response_time",
          "metricValue": 3.306185715994168,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.315Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo2900g0lcdoxhrtwnql",
          "metricName": "pass_rate",
          "metricValue": 0.7683571665131314,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.329Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt2u04b0lcy4ltno5et3",
          "metricName": "elo_rating",
          "metricValue": 1467.344316721686,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.422Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewuboev00hqlcdoubo4s4y2",
          "metricName": "accuracy",
          "metricValue": 0.9819935976265319,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.783Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuboqx00jglcdol6vzoizb",
          "metricName": "accuracy",
          "metricValue": 0.6355066460989532,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.218Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp3200l6lcdoajrpcgw2",
          "metricName": "accuracy",
          "metricValue": 0.6634652947617398,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.654Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpeh00mwlcdo0a4tkxdi",
          "metricName": "accuracy",
          "metricValue": 0.8802400935553543,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.065Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpr400omlcdobia9f8z0",
          "metricName": "overall_score",
          "metricValue": 8.579194780734852,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.520Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqsn00tslcdorzxwbmui",
          "metricName": "average_score",
          "metricValue": 65.69683771383784,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.872Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr5k00vilcdorsvq58mb",
          "metricName": "win_rate",
          "metricValue": 0.7479195948799711,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.336Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j2j039blcy4q8jjpugk",
          "metricName": "accuracy",
          "metricValue": 0.5434958074143295,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.923Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jm503cllcy45ukq9kmp",
          "metricName": "success_rate",
          "metricValue": 0.3288833954181209,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.630Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k5g03fvlcy41w1zer03",
          "metricName": "overall_score",
          "metricValue": 61.72988707115119,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.324Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kp103j5lcy4p83wsacm",
          "metricName": "accuracy",
          "metricValue": 0.5237805079279989,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.029Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l8y03mflcy4atgkvl4e",
          "metricName": "truth_score",
          "metricValue": 0.7191235895620186,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.746Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lsn03pplcy4tu3e3i7i",
          "metricName": "accuracy",
          "metricValue": 0.3012255856981487,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.455Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtk304ealcy471qv1huq",
          "metricName": "percentile_rank",
          "metricValue": 60.60509523784459,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.044Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.289Z"
    },
    {
      "id": "cmewubd80009jlcdo1zmo9p2y",
      "name": "deepseek-coder-33b",
      "displayName": "DeepSeek Coder 33B",
      "provider": "DeepSeek",
      "description": "Specialized coding model from DeepSeek",
      "contextWindow": 32768,
      "pricingInput": 0.00014,
      "pricingOutput": 0.00028,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"code_review\",\"debugging\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.791206936730946,
      "successRate": 0.7422296668286624,
      "passRate": 0.7514718215209362,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubnaq00cilcdosu6qa9vv",
          "metricName": "success_rate",
          "metricValue": 0.7422296668286624,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.339Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnpb00e8lcdo9eoazx4l",
          "metricName": "pass_rate",
          "metricValue": 0.7514718215209362,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.863Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqcr00rmlcdo45dgzicj",
          "metricName": "success_rate",
          "metricValue": 0.945283608392506,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.299Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqcc00rklcdost6b3ciw",
          "metricName": "avg_response_time",
          "metricValue": 2.791206936730946,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.284Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo1v00fylcdo3h0mmfgy",
          "metricName": "pass_rate",
          "metricValue": 0.5871162666401286,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.315Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt2l04aylcy4g7q5s8i9",
          "metricName": "elo_rating",
          "metricValue": 1342.296486989119,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.414Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewuboek00holcdon5ar7unv",
          "metricName": "accuracy",
          "metricValue": 0.7364643566905089,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.772Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuboqk00jelcdo0st7htct",
          "metricName": "accuracy",
          "metricValue": 0.6046985953408341,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.204Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp2p00l4lcdoct9q0v0i",
          "metricName": "accuracy",
          "metricValue": 0.6687601498510205,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.642Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpe400mulcdoempjt7kz",
          "metricName": "accuracy",
          "metricValue": 0.6134561781149781,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.053Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpqp00oklcdont076xbm",
          "metricName": "overall_score",
          "metricValue": 7.627823223343222,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.505Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqs700tqlcdo30nxzp3v",
          "metricName": "average_score",
          "metricValue": 74.2233195611671,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.855Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr5700vglcdot7n4ydz1",
          "metricName": "win_rate",
          "metricValue": 0.4307055998511438,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.323Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j260399lcy4kurqv1f9",
          "metricName": "accuracy",
          "metricValue": 0.6451278376124938,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.910Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jlt03cjlcy4o13jlpc4",
          "metricName": "success_rate",
          "metricValue": 0.3701472404728133,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.617Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k5303ftlcy4w1s9r62t",
          "metricName": "overall_score",
          "metricValue": 50.56274045352853,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.312Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kop03j3lcy4e5ci28j0",
          "metricName": "accuracy",
          "metricValue": 0.2020371870126929,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.017Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l8m03mdlcy4fv1kroct",
          "metricName": "truth_score",
          "metricValue": 0.6796012763738806,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.734Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lsb03pnlcy49qbixu72",
          "metricName": "accuracy",
          "metricValue": 0.4707290087172876,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.443Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtjt04e8lcy4lk136u2q",
          "metricName": "percentile_rank",
          "metricValue": 58.78310972895562,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.034Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.280Z"
    },
    {
      "id": "cmewubd4q0096lcdo8c7m0erq",
      "name": "gpt-3.5-turbo",
      "displayName": "GPT-3.5 Turbo",
      "provider": "OpenAI",
      "description": "Fast and affordable model",
      "contextWindow": 16385,
      "pricingInput": 0.0005,
      "pricingOutput": 0.0015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"basic_coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.86056237882239,
      "successRate": 0.7817468173193929,
      "passRate": 0.7922648791508607,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn5s00bslcdohvjwmzke",
          "metricName": "success_rate",
          "metricValue": 0.7817468173193929,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.160Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnib00dilcdooj0m6ttd",
          "metricName": "pass_rate",
          "metricValue": 0.7922648791508607,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.611Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq1s00q6lcdofm7od4bk",
          "metricName": "success_rate",
          "metricValue": 0.8816777239022795,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.904Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq1b00q4lcdoy8ezrz3m",
          "metricName": "avg_response_time",
          "metricValue": 2.86056237882239,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.888Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnwg00f8lcdoi6pedixv",
          "metricName": "pass_rate",
          "metricValue": 0.7109864901721098,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.121Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsyw04a8lcy44wqm1yrm",
          "metricName": "elo_rating",
          "metricValue": 1042.96093170037,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.281Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubo9b00gylcdo9q8pqc1h",
          "metricName": "accuracy",
          "metricValue": 0.746588012378626,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.584Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubolh00iolcdoh1oacnhr",
          "metricName": "accuracy",
          "metricValue": 0.5620751200068123,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.022Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewuboxp00kelcdorw252x4m",
          "metricName": "accuracy",
          "metricValue": 0.687678692693287,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.461Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp9p00m4lcdolwjo930n",
          "metricName": "accuracy",
          "metricValue": 0.8698440310005687,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.893Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpll00nulcdounz7ub7y",
          "metricName": "overall_score",
          "metricValue": 8.935408742497934,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.321Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqmu00t0lcdoq31fcsj3",
          "metricName": "average_score",
          "metricValue": 63.32672339226867,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.662Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubqzo00uqlcdoenexkliy",
          "metricName": "win_rate",
          "metricValue": 0.5664848324960383,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.125Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ixl038jlcy46q1dgemj",
          "metricName": "accuracy",
          "metricValue": 0.5707704898855616,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.746Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jhd03btlcy4lgxfhyci",
          "metricName": "success_rate",
          "metricValue": 0.3791562301844436,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.457Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k0s03f3lcy43fdf0pe6",
          "metricName": "overall_score",
          "metricValue": 64.94176617276992,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.156Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kkg03idlcy4fcip9bxd",
          "metricName": "accuracy",
          "metricValue": 0.6795923864653854,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.864Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l4703lnlcy45xnlwi7u",
          "metricName": "truth_score",
          "metricValue": 0.6824727024724159,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.575Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lo903oxlcy4jvda3lbe",
          "metricName": "accuracy",
          "metricValue": 0.5273249900553814,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.297Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtg704dilcy48tv1s1sr",
          "metricName": "percentile_rank",
          "metricValue": 25.87306075019283,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.903Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.162Z"
    },
    {
      "id": "cmewubd4f0095lcdobbzfbmm9",
      "name": "gpt-4",
      "displayName": "GPT-4",
      "provider": "OpenAI",
      "description": "Original GPT-4 model",
      "contextWindow": 8192,
      "pricingInput": 0.03,
      "pricingOutput": 0.06,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.961461974884975,
      "successRate": 0.9344803820658152,
      "passRate": 0.8479045177436085,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn5g00bqlcdodpf2s78o",
          "metricName": "success_rate",
          "metricValue": 0.9344803820658152,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.149Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnhr00dglcdowil3fsqm",
          "metricName": "pass_rate",
          "metricValue": 0.8479045177436085,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.591Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq0r00q2lcdo3asx9nue",
          "metricName": "success_rate",
          "metricValue": 0.7511098498277019,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.868Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq0d00q0lcdo86plizp8",
          "metricName": "avg_response_time",
          "metricValue": 3.961461974884975,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.853Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnw100f6lcdov5vre05m",
          "metricName": "pass_rate",
          "metricValue": 0.6332428599130911,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.105Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsyo04a6lcy40vfy1vks",
          "metricName": "elo_rating",
          "metricValue": 950.0302702589669,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.272Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubo8z00gwlcdoxbjemtou",
          "metricName": "accuracy",
          "metricValue": 0.8401589080468419,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.571Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubol400imlcdor7j4h0v3",
          "metricName": "accuracy",
          "metricValue": 0.9510332432076967,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.008Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubox900kclcdoop2hv8im",
          "metricName": "accuracy",
          "metricValue": 0.9503585697375279,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.445Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp9b00m2lcdov7pvijwk",
          "metricName": "accuracy",
          "metricValue": 0.7162312347107658,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.879Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpl600nslcdoxskcwe60",
          "metricName": "overall_score",
          "metricValue": 6.198924446240763,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.306Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqmf00sylcdowncimybd",
          "metricName": "average_score",
          "metricValue": 76.57819056030564,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.648Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubqz900uolcdoeh15wtz2",
          "metricName": "win_rate",
          "metricValue": 0.4589923119299736,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.109Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ix8038hlcy4juimewtw",
          "metricName": "accuracy",
          "metricValue": 0.5519730425048314,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.732Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jh103brlcy42v33c0bn",
          "metricName": "success_rate",
          "metricValue": 0.4611709967987196,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.446Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k0h03f1lcy4xr2q4rkl",
          "metricName": "overall_score",
          "metricValue": 61.56801929077584,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.145Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kk503iblcy4catgzvx7",
          "metricName": "accuracy",
          "metricValue": 0.6537449596464804,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.854Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l3t03lllcy41cv094xq",
          "metricName": "truth_score",
          "metricValue": 0.5851323118702756,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.562Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lny03ovlcy4nn1zs58n",
          "metricName": "accuracy",
          "metricValue": 0.3791291607802417,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.286Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtfx04dglcy4qtvwdxop",
          "metricName": "percentile_rank",
          "metricValue": 64.99813431212479,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.893Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.152Z"
    },
    {
      "id": "cmewubd460094lcdotvna1bne",
      "name": "gpt-4-turbo",
      "displayName": "GPT-4 Turbo",
      "provider": "OpenAI",
      "description": "Advanced model with large context window",
      "contextWindow": 128000,
      "pricingInput": 0.01,
      "pricingOutput": 0.03,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"reasoning\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.899019203931997,
      "successRate": 0.7895252608524055,
      "passRate": 0.7408000796884452,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn5600bolcdoehhaqixa",
          "metricName": "success_rate",
          "metricValue": 0.7895252608524055,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.138Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnh600delcdoe6hdhns1",
          "metricName": "pass_rate",
          "metricValue": 0.7408000796884452,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.570Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq0000pylcdoixu7l0n0",
          "metricName": "success_rate",
          "metricValue": 0.8635453571977623,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.840Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubpzo00pwlcdoy9hwwpqt",
          "metricName": "avg_response_time",
          "metricValue": 3.899019203931997,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.828Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnvk00f4lcdovbe0f1z8",
          "metricName": "pass_rate",
          "metricValue": 0.6774838582990361,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.088Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsyf04a4lcy41frto4qq",
          "metricName": "elo_rating",
          "metricValue": 1149.339252111633,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.263Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubo8m00gulcdocbztzv4u",
          "metricName": "accuracy",
          "metricValue": 0.7796423319008949,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.558Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubokp00iklcdotmye2oce",
          "metricName": "accuracy",
          "metricValue": 0.8774106485513872,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:13.993Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubowt00kalcdo3xogwhno",
          "metricName": "accuracy",
          "metricValue": 0.7466010006539269,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.429Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp8v00m0lcdo85h7n1iw",
          "metricName": "accuracy",
          "metricValue": 0.9753451999803608,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.863Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpkr00nqlcdozd8hjbim",
          "metricName": "overall_score",
          "metricValue": 8.362610545748868,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.292Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqm200swlcdo7w2mt7d2",
          "metricName": "average_score",
          "metricValue": 68.93521367984656,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.634Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubqys00umlcdo3f9ncnlb",
          "metricName": "win_rate",
          "metricValue": 0.4907189809128434,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.092Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13iwv038flcy443hwth2e",
          "metricName": "accuracy",
          "metricValue": 0.5231419603587686,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.719Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jgq03bplcy4yit19ra2",
          "metricName": "success_rate",
          "metricValue": 0.551540534561475,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.434Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k0503ezlcy4jvg3idyg",
          "metricName": "overall_score",
          "metricValue": 58.97662880591744,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.133Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kjt03i9lcy4hm1ajm36",
          "metricName": "accuracy",
          "metricValue": 0.4689307488699754,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.842Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l3i03ljlcy416slbmat",
          "metricName": "truth_score",
          "metricValue": 0.5167383752196067,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.551Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lnm03otlcy49u8jxftu",
          "metricName": "accuracy",
          "metricValue": 0.4963038304093508,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.275Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtfm04delcy4juctt6k0",
          "metricName": "percentile_rank",
          "metricValue": 10.13688360153701,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.882Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.142Z"
    },
    {
      "id": "cmewtbrxg0001lcik3l8ox6c1",
      "name": "gpt-4o",
      "displayName": "GPT-4o",
      "provider": "OpenAI",
      "description": "Fast and capable model for general tasks",
      "contextWindow": 128000,
      "pricingInput": 0.005,
      "pricingOutput": 0.015,
      "isFree": false,
      "isRecommended": true,
      "isAvailableInCursor": false,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"coding\",\"reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.862605544662777,
      "successRate": 0.7436655721243951,
      "passRate": 0.689966176402673,
      "totalRatings": 0,
      "totalBenchmarks": 25,
      "aiderBenchmark": 0.8286846145811589,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewu7tsy0032lcdouuuzjn07",
          "metricName": "success_rate",
          "metricValue": 0.7436655721243951,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T12:57:14.146Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7tun003alcdot4rl3hlp",
          "metricName": "pass_rate",
          "metricValue": 0.689966176402673,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T12:57:14.207Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7u8d004ylcdot30a13y2",
          "metricName": "success_rate",
          "metricValue": 0.7002836746674292,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T12:57:14.701Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewu7u7v004wlcdokx7qfodd",
          "metricName": "avg_response_time",
          "metricValue": 1.862605544662777,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T12:57:14.683Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewu7twf003ilcdoue5gzwdr",
          "metricName": "pass_rate",
          "metricValue": 0.6828897552761967,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T12:57:14.272Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gswb049qlcy4z41i7vbo",
          "metricName": "elo_rating",
          "metricValue": 1095.46990615579,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.188Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewu7ty5003qlcdoqyyc41g1",
          "metricName": "accuracy",
          "metricValue": 0.9174400072250257,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T12:57:14.333Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewu7tzs003ylcdojd8trnbf",
          "metricName": "accuracy",
          "metricValue": 0.7517949407097637,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T12:57:14.392Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewu7u1n0046lcdohswf58h0",
          "metricName": "accuracy",
          "metricValue": 0.628970914335287,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T12:57:14.459Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewu7u3i004elcdo6zrlz8vc",
          "metricName": "accuracy",
          "metricValue": 0.6893706025569879,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T12:57:14.526Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewu7u5h004mlcdoap5qdycn",
          "metricName": "overall_score",
          "metricValue": 9.842015546891028,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T12:57:14.598Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewu7ubc005alcdo3176beqp",
          "metricName": "average_score",
          "metricValue": 76.06999502662731,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T12:57:14.808Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewu7udf005ilcdostcbyhf9",
          "metricName": "win_rate",
          "metricValue": 0.6283541338244595,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T12:57:14.884Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewx9b3q000clcig9m5gutlr",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.8286846145811589,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:22:22.070Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmewwqt6e0008lcic1k7fjcl6",
          "metricName": "pass_rate",
          "metricValue": 0.886260772762179,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T14:07:59.030Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmewwqt660006lciczlig7rsk",
          "metricName": "avg_response_time",
          "metricValue": 2.336625529458766,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T14:07:59.022Z",
          "source": {
            "id": "cmewwq45j0000lch8eib5cykx",
            "name": "test_source",
            "displayName": "Test Benchmark Source",
            "description": "Тестовый источник данных для демонстрации сортировки",
            "category": "independent"
          }
        },
        {
          "id": "cmewxzjnr000clcj0adgawvn1",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.7559999999999999,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.215Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13iuh0381lcy49oo9dxjv",
          "metricName": "accuracy",
          "metricValue": 0.7848358538675877,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.633Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jee03bblcy4o60arkmd",
          "metricName": "success_rate",
          "metricValue": 0.4363210730148939,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.351Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13jxv03ellcy4oc2ek21z",
          "metricName": "overall_score",
          "metricValue": 66.2078863632593,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.052Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13khj03hvlcy4zfg4cl9w",
          "metricName": "accuracy",
          "metricValue": 0.5140849696345227,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.759Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l1703l5lcy4iu12su6n",
          "metricName": "truth_score",
          "metricValue": 0.5071869247762151,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.467Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13llb03oflcy41h0eictd",
          "metricName": "accuracy",
          "metricValue": 0.4257286677198708,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.191Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e12n000ilc1odg334he5",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.7559999999999999,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.815Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtde04d0lcy4bd0xtmy7",
          "metricName": "percentile_rank",
          "metricValue": 30.00391597768615,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.803Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:32:18.725Z"
    },
    {
      "id": "cmewubd3w0093lcdo5zqohcd0",
      "name": "gpt-4o-mini",
      "displayName": "GPT-4o Mini",
      "provider": "OpenAI",
      "description": "Affordable and intelligent small model",
      "contextWindow": 128000,
      "pricingInput": 0.00015,
      "pricingOutput": 0.0006,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"basic_coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.290200977375012,
      "successRate": 0.882278832968949,
      "passRate": 0.7167316262497174,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn4u00bmlcdoqj39yfq1",
          "metricName": "success_rate",
          "metricValue": 0.882278832968949,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.126Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubngp00dclcdouu48vx6j",
          "metricName": "pass_rate",
          "metricValue": 0.7167316262497174,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.553Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubpza00pulcdo5za24wuw",
          "metricName": "success_rate",
          "metricValue": 0.9456542564888236,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.814Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubpyw00pslcdoqsjp59l2",
          "metricName": "avg_response_time",
          "metricValue": 1.290200977375012,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.800Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnv700f2lcdonapu6pqj",
          "metricName": "pass_rate",
          "metricValue": 0.7561334568755123,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.075Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsy404a2lcy4h8yah1v9",
          "metricName": "elo_rating",
          "metricValue": 1678.797541287261,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.252Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubo8600gslcdo7i251rnb",
          "metricName": "accuracy",
          "metricValue": 0.7200381197910899,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.542Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubok900iilcdox9iv0w93",
          "metricName": "accuracy",
          "metricValue": 0.8355209153089456,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:13.977Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubowd00k8lcdokg4oaan0",
          "metricName": "accuracy",
          "metricValue": 0.7881734599247241,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.413Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp8h00lylcdodx1iiy5j",
          "metricName": "accuracy",
          "metricValue": 0.8360938724177434,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.849Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpkb00nolcdomnx3s5xj",
          "metricName": "overall_score",
          "metricValue": 7.909104644888093,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.275Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqlq00sulcdonpwgjf4f",
          "metricName": "average_score",
          "metricValue": 67.96527881111633,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.622Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubqyf00uklcdow5mct0av",
          "metricName": "win_rate",
          "metricValue": 0.6147986044909518,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.079Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13iwh038dlcy4xa55diet",
          "metricName": "accuracy",
          "metricValue": 0.7651050135212687,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.705Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jgf03bnlcy4y8r720lr",
          "metricName": "success_rate",
          "metricValue": 0.3140937549901147,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.423Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13jzt03exlcy4x5w9f8bp",
          "metricName": "overall_score",
          "metricValue": 76.72323047045211,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.122Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kjh03i7lcy4vyfjktwq",
          "metricName": "accuracy",
          "metricValue": 0.5293898016479615,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.830Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l3603lhlcy4n1e42j9n",
          "metricName": "truth_score",
          "metricValue": 0.5593886849728772,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.539Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lnb03orlcy4cztwzhgw",
          "metricName": "accuracy",
          "metricValue": 0.4719327880008518,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.263Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtfb04dclcy46vu15gy4",
          "metricName": "percentile_rank",
          "metricValue": 63.82612943186132,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.871Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.132Z"
    },
    {
      "id": "cmewubd5a0098lcdo22i9w021",
      "name": "gemini-1.5-flash",
      "displayName": "Gemini 1.5 Flash",
      "provider": "Google",
      "description": "Fast multimodal model from Google",
      "contextWindow": 1048576,
      "pricingInput": 0.000075,
      "pricingOutput": 0.0003,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"multimodal\",\"basic_coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.518501389850008,
      "successRate": 0.885570563371044,
      "passRate": 0.7333384882303934,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn6g00bwlcdo4apwwi2n",
          "metricName": "success_rate",
          "metricValue": 0.885570563371044,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.184Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnji00dmlcdo1f2p6tq8",
          "metricName": "pass_rate",
          "metricValue": 0.7333384882303934,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.654Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq3w00qelcdotaqr6ot4",
          "metricName": "success_rate",
          "metricValue": 0.7198866812042474,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.981Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq3j00qclcdoi4uqvch6",
          "metricName": "avg_response_time",
          "metricValue": 2.518501389850008,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.967Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnx900fclcdoqduzk49n",
          "metricName": "pass_rate",
          "metricValue": 0.5878445825231068,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.150Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gszk04aclcy4ggk23ti5",
          "metricName": "elo_rating",
          "metricValue": 1029.360479437427,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.304Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewuboa100h2lcdoivhsrwzi",
          "metricName": "accuracy",
          "metricValue": 0.8310253427908071,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.609Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuboma00islcdo9naghyap",
          "metricName": "accuracy",
          "metricValue": 0.8613375819903714,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.051Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewuboyk00kilcdo6c07h8dz",
          "metricName": "accuracy",
          "metricValue": 0.7719264122920333,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.492Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpac00m8lcdo07gpcbow",
          "metricName": "accuracy",
          "metricValue": 0.6506703191802874,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.917Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpmh00nylcdouxmvjk9m",
          "metricName": "overall_score",
          "metricValue": 9.934249378914775,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.353Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqnp00t4lcdocyry76w0",
          "metricName": "average_score",
          "metricValue": 76.80083942181815,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.693Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr0h00uulcdoo37h1dgo",
          "metricName": "win_rate",
          "metricValue": 0.5669151932641512,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.154Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13iya038nlcy46m0t9o0k",
          "metricName": "accuracy",
          "metricValue": 0.5214925461645219,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.771Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13ji203bxlcy4zvd1hgr8",
          "metricName": "success_rate",
          "metricValue": 0.4610677874920691,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.482Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k1h03f7lcy4ax55xmqr",
          "metricName": "overall_score",
          "metricValue": 69.54178422628371,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.181Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kl303ihlcy4c2lja5px",
          "metricName": "accuracy",
          "metricValue": 0.2681066217630095,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.887Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l4w03lrlcy4g0g4vjvn",
          "metricName": "truth_score",
          "metricValue": 0.7643455995154473,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.600Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lov03p1lcy4cxsmu9nn",
          "metricName": "accuracy",
          "metricValue": 0.453250171404744,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.320Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtgs04dmlcy4besghhbx",
          "metricName": "percentile_rank",
          "metricValue": 51.46137817374076,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.924Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.182Z"
    },
    {
      "id": "cmewubd500097lcdo4mddjx0m",
      "name": "gemini-1.5-pro",
      "displayName": "Gemini 1.5 Pro",
      "provider": "Google",
      "description": "Advanced multimodal model from Google",
      "contextWindow": 2097152,
      "pricingInput": 0.00025,
      "pricingOutput": 0.0005,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"reasoning\",\"multimodal\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.465142250694233,
      "successRate": 0.8192702574061572,
      "passRate": 0.8054646331379487,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn6300bulcdot4fxvtvd",
          "metricName": "success_rate",
          "metricValue": 0.8192702574061572,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.171Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnj000dklcdo9kf7sx72",
          "metricName": "pass_rate",
          "metricValue": 0.8054646331379487,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.636Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq3300qalcdozkypoao2",
          "metricName": "success_rate",
          "metricValue": 0.7879948983767541,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:15.951Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq2800q8lcdogyxzqfz0",
          "metricName": "avg_response_time",
          "metricValue": 1.465142250694233,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.920Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnwu00falcdodjgz39an",
          "metricName": "pass_rate",
          "metricValue": 0.5280508109762907,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.134Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gsz904aalcy4btbnnwb6",
          "metricName": "elo_rating",
          "metricValue": 1650.082843210911,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.293Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubo9n00h0lcdo6u56y3ds",
          "metricName": "accuracy",
          "metricValue": 0.7659380768539591,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.595Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubolw00iqlcdosleol1ft",
          "metricName": "accuracy",
          "metricValue": 0.938936270925098,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.036Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewuboy500kglcdoguqpb9l1",
          "metricName": "accuracy",
          "metricValue": 0.8286546557542078,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.477Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpa100m6lcdojjrf46un",
          "metricName": "accuracy",
          "metricValue": 0.7322538490395508,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.905Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpm100nwlcdo64zck4th",
          "metricName": "overall_score",
          "metricValue": 7.628449944438869,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.337Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqn900t2lcdonatqlinw",
          "metricName": "average_score",
          "metricValue": 74.92539966258559,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.677Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr0200uslcdoayfhvemy",
          "metricName": "win_rate",
          "metricValue": 0.6812380468581001,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.139Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ixy038llcy4lr1qxyhj",
          "metricName": "accuracy",
          "metricValue": 0.7045746375109554,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.759Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jhp03bvlcy4wxr94zb9",
          "metricName": "success_rate",
          "metricValue": 0.4643327156183882,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.470Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k1403f5lcy4z6hqpy4z",
          "metricName": "overall_score",
          "metricValue": 55.87136427464059,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.169Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kks03iflcy4x60xw999",
          "metricName": "accuracy",
          "metricValue": 0.4277855628998788,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.876Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l4k03lplcy4vjkxobly",
          "metricName": "truth_score",
          "metricValue": 0.6620092741571295,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.588Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lok03ozlcy4oufen86m",
          "metricName": "accuracy",
          "metricValue": 0.3002141638603705,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.308Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtgh04dklcy42xketa4k",
          "metricName": "percentile_rank",
          "metricValue": 20.06884717910314,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.914Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.173Z"
    },
    {
      "id": "cmewubd5k0099lcdo4undaa7i",
      "name": "gemini-pro",
      "displayName": "Gemini Pro",
      "provider": "Google",
      "description": "Google's advanced language model",
      "contextWindow": 30720,
      "pricingInput": 0.00025,
      "pricingOutput": 0.0005,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"basic_coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.685463189511201,
      "successRate": 0.8606833876038743,
      "passRate": 0.7708538458259276,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn6r00bylcdo2h2ef9yf",
          "metricName": "success_rate",
          "metricValue": 0.8606833876038743,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.196Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnk000dolcdo1wc1wt2g",
          "metricName": "pass_rate",
          "metricValue": 0.7708538458259276,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.672Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq4m00qilcdozivvnle4",
          "metricName": "success_rate",
          "metricValue": 0.959518301898213,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.006Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq4a00qglcdo64m7xyok",
          "metricName": "avg_response_time",
          "metricValue": 1.685463189511201,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:15.994Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnxn00felcdoaql7phdq",
          "metricName": "pass_rate",
          "metricValue": 0.7722231112185163,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.163Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gszu04aelcy4wog8o8q3",
          "metricName": "elo_rating",
          "metricValue": 1513.061678012474,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.315Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewuboag00h4lcdoba2oub49",
          "metricName": "accuracy",
          "metricValue": 0.8960480496201022,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.624Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubomp00iulcdo861bqgp7",
          "metricName": "accuracy",
          "metricValue": 0.5987856477205205,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.065Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewuboyx00kklcdooj37adyv",
          "metricName": "accuracy",
          "metricValue": 0.9978714168282619,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.505Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpap00malcdo7wmja3sj",
          "metricName": "accuracy",
          "metricValue": 0.6548260090545998,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.929Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpmx00o0lcdo06vsvydw",
          "metricName": "overall_score",
          "metricValue": 8.790725429701304,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.369Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqo300t6lcdohjdgceqt",
          "metricName": "average_score",
          "metricValue": 78.94716594934013,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.707Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr0w00uwlcdojtlw3xkr",
          "metricName": "win_rate",
          "metricValue": 0.4017187820873565,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.168Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13iym038plcy4v8zy68pu",
          "metricName": "accuracy",
          "metricValue": 0.4859417573573885,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.782Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jif03bzlcy45ml3m179",
          "metricName": "success_rate",
          "metricValue": 0.3062770141331586,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.496Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k1s03f9lcy4gov4i2kp",
          "metricName": "overall_score",
          "metricValue": 54.04518357768122,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.193Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kle03ijlcy4bx3wxay2",
          "metricName": "accuracy",
          "metricValue": 0.5017539509840785,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.898Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l5903ltlcy4j1h89y7m",
          "metricName": "truth_score",
          "metricValue": 0.8955818121340702,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.613Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lp703p3lcy4244vduqt",
          "metricName": "accuracy",
          "metricValue": 0.5994936494196755,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.331Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gth304dolcy47v0b6cjv",
          "metricName": "percentile_rank",
          "metricValue": 52.86126913139356,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.935Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.193Z"
    },
    {
      "id": "cmewubd9t009qlcdowiaj3nzo",
      "name": "grok-1",
      "displayName": "Grok 1",
      "provider": "xAI",
      "description": "Original Grok model",
      "contextWindow": 128000,
      "pricingInput": 0.0008,
      "pricingOutput": 0.004,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"reasoning\",\"humor\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.199900035906502,
      "successRate": 0.8252439709753099,
      "passRate": 0.602068069926996,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubncv00cwlcdoytzs6vgj",
          "metricName": "success_rate",
          "metricValue": 0.8252439709753099,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.415Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubns700emlcdojzamu7ww",
          "metricName": "pass_rate",
          "metricValue": 0.602068069926996,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.968Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqil00selcdo2cdavy1e",
          "metricName": "success_rate",
          "metricValue": 0.8101487013966661,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.510Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqi700sclcdo3sqybnoo",
          "metricName": "avg_response_time",
          "metricValue": 1.199900035906502,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.496Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo4t00gclcdoxaz8eznq",
          "metricName": "pass_rate",
          "metricValue": 0.6546865260401715,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.422Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt4q04bclcy409gil7ae",
          "metricName": "elo_rating",
          "metricValue": 1513.633303483751,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.491Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubogx00i2lcdob8uk1gjl",
          "metricName": "accuracy",
          "metricValue": 0.7369979526336269,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.857Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubotc00jslcdogusnu3sw",
          "metricName": "accuracy",
          "metricValue": 0.9072523014437363,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.304Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp5h00lilcdotm0rak59",
          "metricName": "accuracy",
          "metricValue": 0.6101983591854628,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.741Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpgx00n8lcdoezt79mxh",
          "metricName": "accuracy",
          "metricValue": 0.6952973680974861,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.153Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpt900oylcdoruztuu0e",
          "metricName": "overall_score",
          "metricValue": 7.591591798079475,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.597Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubquy00u4lcdo0ql4f4kw",
          "metricName": "average_score",
          "metricValue": 61.81163257699893,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.954Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr7w00vulcdo0nh80us0",
          "metricName": "win_rate",
          "metricValue": 0.5963211255343487,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.420Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j4h039nlcy43g9lvudc",
          "metricName": "accuracy",
          "metricValue": 0.5557762897675302,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.993Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jo103cxlcy4wmgf8gmv",
          "metricName": "success_rate",
          "metricValue": 0.450441045183822,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.698Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k7n03g7lcy4rfjmtl5n",
          "metricName": "overall_score",
          "metricValue": 70.25688513171951,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.403Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kr703jhlcy46jyv5jiy",
          "metricName": "accuracy",
          "metricValue": 0.4013377874150029,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.107Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lb003mrlcy4uc5cas8g",
          "metricName": "truth_score",
          "metricValue": 0.6851718024015312,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.820Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13luk03q1lcy4f49uw3fc",
          "metricName": "accuracy",
          "metricValue": 0.3439597746744211,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.524Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtlr04emlcy4rz6kq63f",
          "metricName": "percentile_rank",
          "metricValue": 45.60547653385437,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.103Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.345Z"
    },
    {
      "id": "cmewubd9l009plcdo7ndfwpfk",
      "name": "grok-2",
      "displayName": "Grok 2",
      "provider": "xAI",
      "description": "xAI's advanced reasoning model",
      "contextWindow": 128000,
      "pricingInput": 0.002,
      "pricingOutput": 0.01,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"reasoning\",\"humor\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.068786162961057,
      "successRate": 0.9179374698080123,
      "passRate": 0.869095067710355,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubncj00culcdotbrqk7cg",
          "metricName": "success_rate",
          "metricValue": 0.9179374698080123,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.404Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnrt00eklcdo1oo21atr",
          "metricName": "pass_rate",
          "metricValue": 0.869095067710355,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.954Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqht00salcdozm03m0qh",
          "metricName": "success_rate",
          "metricValue": 0.9682989822610457,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.481Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqhf00s8lcdopagdtjob",
          "metricName": "avg_response_time",
          "metricValue": 2.068786162961057,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.467Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo4e00galcdoad2g500o",
          "metricName": "pass_rate",
          "metricValue": 0.7772418815963016,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.406Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt4h04balcy40ehoypln",
          "metricName": "elo_rating",
          "metricValue": 1779.197093129732,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.481Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubogi00i0lcdosapmve0j",
          "metricName": "accuracy",
          "metricValue": 0.9308822882612234,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.842Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubosv00jqlcdovqcs93vq",
          "metricName": "accuracy",
          "metricValue": 0.8098716652061618,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.288Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp5300lglcdo4t3ukypk",
          "metricName": "accuracy",
          "metricValue": 0.6898276136803412,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.727Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpgi00n6lcdosopnaboh",
          "metricName": "accuracy",
          "metricValue": 0.7856331652941813,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.138Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpsx00owlcdofhey95z8",
          "metricName": "overall_score",
          "metricValue": 9.47115558507922,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.586Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqum00u2lcdo42lgldh2",
          "metricName": "average_score",
          "metricValue": 64.78705796406328,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.942Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr7f00vslcdojzriauqo",
          "metricName": "win_rate",
          "metricValue": 0.5420433839734522,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.403Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j46039llcy4r5m876ez",
          "metricName": "accuracy",
          "metricValue": 0.616984476892644,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.982Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jnq03cvlcy4n1tmdpfy",
          "metricName": "success_rate",
          "metricValue": 0.4868604886242455,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.686Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k7c03g5lcy4e9fm3uv3",
          "metricName": "overall_score",
          "metricValue": 70.6461579006706,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.392Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kqu03jflcy47x5y2yat",
          "metricName": "accuracy",
          "metricValue": 0.2200589852957287,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.095Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lal03mplcy4n0nbcu9e",
          "metricName": "truth_score",
          "metricValue": 0.5466448749316164,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.806Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lu703pzlcy4glm9jxte",
          "metricName": "accuracy",
          "metricValue": 0.4941328668714009,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.512Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtlg04eklcy47ewg353n",
          "metricName": "percentile_rank",
          "metricValue": 62.84329603111075,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.093Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.337Z"
    },
    {
      "id": "cmewubd91009nlcdooita2fc6",
      "name": "fireworks-llama-3.1-70b",
      "displayName": "Llama 3.1 70B (Fireworks)",
      "provider": "Fireworks",
      "description": "Optimized Llama model on Fireworks",
      "contextWindow": 131072,
      "pricingInput": 0.0009,
      "pricingOutput": 0.0009,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"reasoning\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.056301141048233,
      "successRate": 0.7453703832136944,
      "passRate": 0.6115470378134508,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubnbx00cqlcdo4hlrsxu1",
          "metricName": "success_rate",
          "metricValue": 0.7453703832136944,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.382Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnr200eglcdos2z2rl0o",
          "metricName": "pass_rate",
          "metricValue": 0.6115470378134508,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.926Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqg600s2lcdocysw5etr",
          "metricName": "success_rate",
          "metricValue": 0.9916562532725588,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.422Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqfq00s0lcdouufslb97",
          "metricName": "avg_response_time",
          "metricValue": 3.056301141048233,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.407Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo3f00g6lcdon9a2ib4r",
          "metricName": "pass_rate",
          "metricValue": 0.8275743091825127,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.371Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt3u04b6lcy4fjfo48lm",
          "metricName": "elo_rating",
          "metricValue": 1549.439085183712,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.459Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubofu00hwlcdor12ttuk2",
          "metricName": "accuracy",
          "metricValue": 0.9930084700215256,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.819Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubos100jmlcdo86bwtvq1",
          "metricName": "accuracy",
          "metricValue": 0.8484758428117652,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.257Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp4700lclcdo0myp2j4a",
          "metricName": "accuracy",
          "metricValue": 0.8686270894563979,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.695Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpfq00n2lcdox651ajqc",
          "metricName": "accuracy",
          "metricValue": 0.6568004363734341,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.110Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubps900oslcdoy2rp0bh1",
          "metricName": "overall_score",
          "metricValue": 7.029774750035352,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.561Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqtv00tylcdo57ukyyni",
          "metricName": "average_score",
          "metricValue": 69.58991073451841,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.915Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr6p00volcdo4sjb2gkt",
          "metricName": "win_rate",
          "metricValue": 0.6996256115701169,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.378Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j3k039hlcy419xiu87q",
          "metricName": "accuracy",
          "metricValue": 0.6392962000492988,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.961Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jn303crlcy405g8z73j",
          "metricName": "success_rate",
          "metricValue": 0.3203108047698153,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.663Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k6n03g1lcy4lltm0nc2",
          "metricName": "overall_score",
          "metricValue": 68.73897080594244,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.367Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kq103jblcy49gq65fjr",
          "metricName": "accuracy",
          "metricValue": 0.4174379622954837,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.066Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l9x03mllcy46gk1457l",
          "metricName": "truth_score",
          "metricValue": 0.5544281773893102,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.781Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13ltl03pvlcy42s3tnljb",
          "metricName": "accuracy",
          "metricValue": 0.5768211857998398,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.489Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtkw04eglcy4y184y8bn",
          "metricName": "percentile_rank",
          "metricValue": 33.04011735949614,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.073Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.317Z"
    },
    {
      "id": "cmewubd70009flcdo0hwkzk4j",
      "name": "llama-3.2-11b",
      "displayName": "Llama 3.2 11B",
      "provider": "Meta",
      "description": "Efficient Llama model for general tasks",
      "contextWindow": 128000,
      "pricingInput": 0.00015,
      "pricingOutput": 0.00015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"basic_coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.516125335266896,
      "successRate": 0.7376566328842534,
      "passRate": 0.8895615380131754,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn8p00calcdo3pc3hokm",
          "metricName": "success_rate",
          "metricValue": 0.7376566328842534,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.266Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnnb00e0lcdoqpuprbbr",
          "metricName": "pass_rate",
          "metricValue": 0.8895615380131754,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.792Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq9f00r6lcdo7b0mr7io",
          "metricName": "success_rate",
          "metricValue": 0.7862025975784187,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.179Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq9100r4lcdohnqds02j",
          "metricName": "avg_response_time",
          "metricValue": 3.516125335266896,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.165Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo0200fqlcdo0lz0v20f",
          "metricName": "pass_rate",
          "metricValue": 0.7979104825111976,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.250Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt1h04aqlcy4r8jmbldk",
          "metricName": "elo_rating",
          "metricValue": 815.1966748242091,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.374Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubocx00hglcdokp6e474s",
          "metricName": "accuracy",
          "metricValue": 0.9472098394881935,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.713Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubop000j6lcdou8fk4iip",
          "metricName": "accuracy",
          "metricValue": 0.6134148136938757,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.149Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp1500kwlcdo7p61ivwq",
          "metricName": "accuracy",
          "metricValue": 0.6408411400254445,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.586Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpcq00mmlcdozjjjnxwa",
          "metricName": "accuracy",
          "metricValue": 0.6499487089601138,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.003Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpp800oclcdo7fzlakiw",
          "metricName": "overall_score",
          "metricValue": 8.358244996901286,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.453Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqqi00tilcdop9bjkux8",
          "metricName": "average_score",
          "metricValue": 67.49990566860411,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.794Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr3f00v8lcdo9e01gwo9",
          "metricName": "win_rate",
          "metricValue": 0.5698851848642527,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.259Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j0u0391lcy4kdgxtrzq",
          "metricName": "accuracy",
          "metricValue": 0.4106697011747862,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.862Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jke03cblcy4jvvmmhor",
          "metricName": "success_rate",
          "metricValue": 0.5968701239472336,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.566Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k3q03fllcy4iyejhcy6",
          "metricName": "overall_score",
          "metricValue": 66.34099557764333,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.262Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kne03ivlcy4ttmo5w8f",
          "metricName": "accuracy",
          "metricValue": 0.4613566873485278,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.970Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l7903m5lcy4oc7lprmd",
          "metricName": "truth_score",
          "metricValue": 0.655398993533258,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.685Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lr103pflcy42srrpriw",
          "metricName": "accuracy",
          "metricValue": 0.496292417144569,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.397Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtip04e0lcy46oelzj4a",
          "metricName": "percentile_rank",
          "metricValue": 50.16346416962229,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.993Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.245Z"
    },
    {
      "id": "cmewubd7a009glcdohd3em3e9",
      "name": "llama-3.2-3b",
      "displayName": "Llama 3.2 3B",
      "provider": "Meta",
      "description": "Lightweight Llama model",
      "contextWindow": 128000,
      "pricingInput": 0.00006,
      "pricingOutput": 0.00006,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"basic_coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.348462444603053,
      "successRate": 0.7998413755897315,
      "passRate": 0.7188159106994119,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn9100cclcdotavz1286",
          "metricName": "success_rate",
          "metricValue": 0.7998413755897315,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.278Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnnu00e2lcdo6wozxak3",
          "metricName": "pass_rate",
          "metricValue": 0.7188159106994119,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.810Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqa600ralcdo0vodv32j",
          "metricName": "success_rate",
          "metricValue": 0.8358221175344042,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.206Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq9s00r8lcdoj5pfa3c7",
          "metricName": "avg_response_time",
          "metricValue": 3.348462444603053,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.192Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo0j00fslcdoyrduvzgk",
          "metricName": "pass_rate",
          "metricValue": 0.8725948368802274,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.267Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt1s04aslcy4ebmp1zb7",
          "metricName": "elo_rating",
          "metricValue": 952.1316692917653,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.385Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubodd00hilcdo79ve7fsh",
          "metricName": "accuracy",
          "metricValue": 0.7148747826369797,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.729Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuboph00j8lcdon4qedhmw",
          "metricName": "accuracy",
          "metricValue": 0.5125464138267448,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.165Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp1i00kylcdosi2egcvj",
          "metricName": "accuracy",
          "metricValue": 0.6101712719930805,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.598Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpd400molcdona62qfqj",
          "metricName": "accuracy",
          "metricValue": 0.7907162979617449,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.016Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubppn00oelcdod8kt190y",
          "metricName": "overall_score",
          "metricValue": 9.458435894546142,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.468Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqqv00tklcdocot7mnhi",
          "metricName": "average_score",
          "metricValue": 72.13055508801953,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.808Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr3w00valcdovyh0qnp0",
          "metricName": "win_rate",
          "metricValue": 0.7772685252257742,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.277Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j160393lcy4ep5ke5pe",
          "metricName": "accuracy",
          "metricValue": 0.5563811995400676,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.874Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jkq03cdlcy4d7zyaes3",
          "metricName": "success_rate",
          "metricValue": 0.3441804024419388,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.579Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k4203fnlcy4nj7wb0cq",
          "metricName": "overall_score",
          "metricValue": 67.59142038047821,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.274Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13knq03ixlcy42d47hu7o",
          "metricName": "accuracy",
          "metricValue": 0.6271496218041108,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.983Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l7k03m7lcy44j32sjpu",
          "metricName": "truth_score",
          "metricValue": 0.859077265128014,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.697Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lrc03phlcy4o88ooge9",
          "metricName": "accuracy",
          "metricValue": 0.4842181629346516,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.408Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtiz04e2lcy4nf0xsbd5",
          "metricName": "percentile_rank",
          "metricValue": 56.87132609701956,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.004Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.254Z"
    },
    {
      "id": "cmewubd6r009elcdo4v1d7x6d",
      "name": "llama-3.2-70b",
      "displayName": "Llama 3.2 70B",
      "provider": "Meta",
      "description": "Advanced Llama model",
      "contextWindow": 128000,
      "pricingInput": 0.00054,
      "pricingOutput": 0.00054,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"reasoning\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.869829951427257,
      "successRate": 0.7928283784988438,
      "passRate": 0.8004921022200137,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn8d00c8lcdooixud9e7",
          "metricName": "success_rate",
          "metricValue": 0.7928283784988438,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.253Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnmm00dylcdoafb65ack",
          "metricName": "pass_rate",
          "metricValue": 0.8004921022200137,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.766Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq8n00r2lcdonq3ghl76",
          "metricName": "success_rate",
          "metricValue": 0.8840976047111156,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.151Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq8800r0lcdoxlnl8vso",
          "metricName": "avg_response_time",
          "metricValue": 3.869829951427257,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.136Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnzn00folcdoxov8h2h5",
          "metricName": "pass_rate",
          "metricValue": 0.7017951711790762,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.235Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt1704aolcy4xt47sumg",
          "metricName": "elo_rating",
          "metricValue": 1286.724506290921,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.364Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubocg00helcdosgf1ku2j",
          "metricName": "accuracy",
          "metricValue": 0.8495050732548708,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.696Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuboon00j4lcdo34w074mm",
          "metricName": "accuracy",
          "metricValue": 0.677009749270716,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.135Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp0s00kulcdo6e0y9kvo",
          "metricName": "accuracy",
          "metricValue": 0.9023527575263659,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.572Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpcc00mklcdo13h4of8u",
          "metricName": "accuracy",
          "metricValue": 0.6484307056082854,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.989Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpou00oalcdo0u1s7wxw",
          "metricName": "overall_score",
          "metricValue": 6.8931597987184,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.438Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqq300tglcdog47g2b4u",
          "metricName": "average_score",
          "metricValue": 78.65127562312658,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.779Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr2z00v6lcdodgs5jsdu",
          "metricName": "win_rate",
          "metricValue": 0.5685379232399554,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.243Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j0i038zlcy4sazriwoy",
          "metricName": "accuracy",
          "metricValue": 0.5862888402958107,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.850Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jk303c9lcy4w6yzgezr",
          "metricName": "success_rate",
          "metricValue": 0.3364780299575074,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.555Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k3e03fjlcy4igaoc9pn",
          "metricName": "overall_score",
          "metricValue": 63.63634313977342,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.250Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kn203itlcy4sl0v6ff4",
          "metricName": "accuracy",
          "metricValue": 0.2905687484679934,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.958Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l6w03m3lcy410h79c6d",
          "metricName": "truth_score",
          "metricValue": 0.5362678545968158,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.672Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lqn03pdlcy43wxlqyr3",
          "metricName": "accuracy",
          "metricValue": 0.5470357575424121,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.383Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtie04dylcy4me1ahpmz",
          "metricName": "percentile_rank",
          "metricValue": 23.84841867275528,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.983Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.235Z"
    },
    {
      "id": "cmewubd6i009dlcdosyf1zq3u",
      "name": "llama-3.2-90b",
      "displayName": "Llama 3.2 90B",
      "provider": "Meta",
      "description": "Large Llama model for complex tasks",
      "contextWindow": 128000,
      "pricingInput": 0.00072,
      "pricingOutput": 0.00072,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"reasoning\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.587510104763823,
      "successRate": 0.9365621255545463,
      "passRate": 0.6753740140794876,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn8000c6lcdol39e9h8u",
          "metricName": "success_rate",
          "metricValue": 0.9365621255545463,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.240Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnm300dwlcdopaljv01t",
          "metricName": "pass_rate",
          "metricValue": 0.6753740140794876,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.747Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq7s00qylcdo9hd9lwsm",
          "metricName": "success_rate",
          "metricValue": 0.8101009380131874,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.120Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq7d00qwlcdon4vmoso7",
          "metricName": "avg_response_time",
          "metricValue": 1.587510104763823,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.105Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnz800fmlcdoym908d59",
          "metricName": "pass_rate",
          "metricValue": 0.8382814338108858,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.220Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt0x04amlcy4pwg2aajl",
          "metricName": "elo_rating",
          "metricValue": 1244.34424296585,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.354Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewuboc000hclcdou2jjg3ot",
          "metricName": "accuracy",
          "metricValue": 0.8188607390544385,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.680Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewuboo900j2lcdom4ksx5m6",
          "metricName": "accuracy",
          "metricValue": 0.7946427969444894,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.121Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp0f00kslcdozluqptft",
          "metricName": "accuracy",
          "metricValue": 0.9201913749396607,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.559Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpbz00milcdogq0tuxm7",
          "metricName": "accuracy",
          "metricValue": 0.8954601392194463,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.976Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpoj00o8lcdo83suqiiy",
          "metricName": "overall_score",
          "metricValue": 6.508646545472494,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.427Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqpn00telcdojfmjbga9",
          "metricName": "average_score",
          "metricValue": 67.18839016049776,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.763Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr2l00v4lcdoeck28nlg",
          "metricName": "win_rate",
          "metricValue": 0.4198775695475889,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.230Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j05038xlcy4a6tgo09p",
          "metricName": "accuracy",
          "metricValue": 0.7761567391766379,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.837Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jjq03c7lcy4ddwxpllh",
          "metricName": "success_rate",
          "metricValue": 0.5910927822382257,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.543Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k3403fhlcy4y09ejx25",
          "metricName": "overall_score",
          "metricValue": 68.66132018247674,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.240Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kmn03irlcy46tdo8yt3",
          "metricName": "accuracy",
          "metricValue": 0.3931270261240922,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.943Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l6j03m1lcy40t8ule9q",
          "metricName": "truth_score",
          "metricValue": 0.5280866130950238,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.659Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lqd03pblcy41dtvuydj",
          "metricName": "accuracy",
          "metricValue": 0.3813522316281809,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.373Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gti604dwlcy4hdi8hmh4",
          "metricName": "percentile_rank",
          "metricValue": 22.58184374200878,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.974Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.226Z"
    },
    {
      "id": "cmewubd5t009alcdo183zvdv1",
      "name": "mistral-large",
      "displayName": "Mistral Large",
      "provider": "Mistral",
      "description": "Most capable Mistral model",
      "contextWindow": 128000,
      "pricingInput": 0.002,
      "pricingOutput": 0.006,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"reasoning\",\"documentation\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.726720391812854,
      "successRate": 0.8214341120004731,
      "passRate": 0.751805767868192,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn7200c0lcdo2gjtghcg",
          "metricName": "success_rate",
          "metricValue": 0.8214341120004731,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.207Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnkj00dqlcdop6xscixx",
          "metricName": "pass_rate",
          "metricValue": 0.751805767868192,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.691Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq5c00qmlcdo9i686s8l",
          "metricName": "success_rate",
          "metricValue": 0.8735416235416078,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.032Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq4z00qklcdooe6syu5e",
          "metricName": "avg_response_time",
          "metricValue": 2.726720391812854,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.019Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubny100fglcdoyu2rtomn",
          "metricName": "pass_rate",
          "metricValue": 0.7647987522948707,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.178Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt0404aglcy4gc9t94hn",
          "metricName": "elo_rating",
          "metricValue": 1637.124606249874,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.324Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewuboau00h6lcdomugjb89d",
          "metricName": "accuracy",
          "metricValue": 0.9951759598951762,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.638Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubon100iwlcdoyyfziwcx",
          "metricName": "accuracy",
          "metricValue": 0.6006810868446842,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.077Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewuboza00kmlcdovhjgdp6t",
          "metricName": "accuracy",
          "metricValue": 0.6138438838609309,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.518Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpb200mclcdohnaq5r3d",
          "metricName": "accuracy",
          "metricValue": 0.9006090416892973,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.942Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpne00o2lcdortj4a7ee",
          "metricName": "overall_score",
          "metricValue": 6.052994633990224,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.386Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqog00t8lcdo5qa7e7wt",
          "metricName": "average_score",
          "metricValue": 68.98941298922807,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.721Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr1b00uylcdoty37396r",
          "metricName": "win_rate",
          "metricValue": 0.7709114653427401,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.183Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13iyx038rlcy459b990r5",
          "metricName": "accuracy",
          "metricValue": 0.4005385643346168,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.793Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jir03c1lcy4q8e2f0v8",
          "metricName": "success_rate",
          "metricValue": 0.586437029225524,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.507Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k2503fblcy497sb3a8y",
          "metricName": "overall_score",
          "metricValue": 60.176112177047,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.205Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13klq03illcy4xfqyuyjk",
          "metricName": "accuracy",
          "metricValue": 0.3825102799838465,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.910Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l5l03lvlcy4oa93on05",
          "metricName": "truth_score",
          "metricValue": 0.7203390681203057,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.625Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lpi03p5lcy4pdqf2eax",
          "metricName": "accuracy",
          "metricValue": 0.5242892142023874,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.342Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gthe04dqlcy4h0zlluu8",
          "metricName": "percentile_rank",
          "metricValue": 41.25672241552088,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.946Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.202Z"
    },
    {
      "id": "cmewubd61009blcdovrzdli5u",
      "name": "mistral-medium",
      "displayName": "Mistral Medium",
      "provider": "Mistral",
      "description": "Balanced Mistral model",
      "contextWindow": 32000,
      "pricingInput": 0.00081,
      "pricingOutput": 0.00243,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.166344585048287,
      "successRate": 0.9960437179112116,
      "passRate": 0.8226989876266901,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn7e00c2lcdo6yaj47to",
          "metricName": "success_rate",
          "metricValue": 0.9960437179112116,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.218Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnl200dslcdotp04lnbh",
          "metricName": "pass_rate",
          "metricValue": 0.8226989876266901,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.711Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq6300qqlcdo2btrdyov",
          "metricName": "success_rate",
          "metricValue": 0.7949384253276749,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.060Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq5o00qolcdomf3fzclp",
          "metricName": "avg_response_time",
          "metricValue": 2.166344585048287,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.044Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnye00filcdouaqoyftl",
          "metricName": "pass_rate",
          "metricValue": 0.68615550421979,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.190Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt0d04ailcy484gghy6j",
          "metricName": "elo_rating",
          "metricValue": 1153.955742715754,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.333Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubob800h8lcdobglll6og",
          "metricName": "accuracy",
          "metricValue": 0.9913205979276332,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.653Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubone00iylcdoph7txxvj",
          "metricName": "accuracy",
          "metricValue": 0.9305318497082335,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.090Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubozo00kolcdobfq4emiy",
          "metricName": "accuracy",
          "metricValue": 0.77266559673149,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.533Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpbd00melcdo0y3rkul9",
          "metricName": "accuracy",
          "metricValue": 0.6855410191639411,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.953Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpnt00o4lcdojbfdmmq5",
          "metricName": "overall_score",
          "metricValue": 7.594551306331074,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.401Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqov00talcdo6sg4xrfh",
          "metricName": "average_score",
          "metricValue": 61.60180299094407,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.735Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr1t00v0lcdotc0696rd",
          "metricName": "win_rate",
          "metricValue": 0.4211657079656207,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.201Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13izb038tlcy4lbbxvrii",
          "metricName": "accuracy",
          "metricValue": 0.6589981467749838,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.808Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jj303c3lcy4mlhy1c7q",
          "metricName": "success_rate",
          "metricValue": 0.4011010514084621,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.520Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k2h03fdlcy4uy0wvl2e",
          "metricName": "overall_score",
          "metricValue": 79.74786365412612,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.217Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13km103inlcy4lrwmr937",
          "metricName": "accuracy",
          "metricValue": 0.2750823630806445,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.921Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l5w03lxlcy415nbqheb",
          "metricName": "truth_score",
          "metricValue": 0.8226321429687072,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.636Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lps03p7lcy4wqpjfkw5",
          "metricName": "accuracy",
          "metricValue": 0.5893088298033688,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.353Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtho04dslcy4mki5ot81",
          "metricName": "percentile_rank",
          "metricValue": 16.79026022552982,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.956Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.209Z"
    },
    {
      "id": "cmewubd69009clcdoj3blopmz",
      "name": "mistral-small",
      "displayName": "Mistral Small",
      "provider": "Mistral",
      "description": "Fast and efficient Mistral model",
      "contextWindow": 32000,
      "pricingInput": 0.00027,
      "pricingOutput": 0.00081,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"basic_coding\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.650580198754282,
      "successRate": 0.9797125207083626,
      "passRate": 0.7457947049796386,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubn7p00c4lcdo6g21c388",
          "metricName": "success_rate",
          "metricValue": 0.9797125207083626,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.229Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnlk00dulcdo8cu3fide",
          "metricName": "pass_rate",
          "metricValue": 0.7457947049796386,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.728Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubq6z00qulcdos5r46a7o",
          "metricName": "success_rate",
          "metricValue": 0.8620062973052978,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.091Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubq6l00qslcdo9ue2ufqr",
          "metricName": "avg_response_time",
          "metricValue": 3.650580198754282,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.077Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubnyt00fklcdo1xrkqdzv",
          "metricName": "pass_rate",
          "metricValue": 0.5974731099147785,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.205Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt0m04aklcy45lyvs6kx",
          "metricName": "elo_rating",
          "metricValue": 910.0678338647729,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.342Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubobn00halcdogewchfbf",
          "metricName": "accuracy",
          "metricValue": 0.9646610273147722,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.667Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubonr00j0lcdonwtixl78",
          "metricName": "accuracy",
          "metricValue": 0.7608557667274228,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.103Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp0300kqlcdo7dtqbq38",
          "metricName": "accuracy",
          "metricValue": 0.8206233801706553,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.547Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpbo00mglcdofq7fp4o0",
          "metricName": "accuracy",
          "metricValue": 0.7463616697034774,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:14.965Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpo700o6lcdoiz7sdhlf",
          "metricName": "overall_score",
          "metricValue": 9.625005702642447,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.416Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqp900tclcdoj29xohto",
          "metricName": "average_score",
          "metricValue": 67.80046410451689,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.749Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr2700v2lcdop7bkzn8x",
          "metricName": "win_rate",
          "metricValue": 0.4154084772160818,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.215Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13izn038vlcy4715hjxb3",
          "metricName": "accuracy",
          "metricValue": 0.6693615658333547,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.820Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jjf03c5lcy4g3pabrs8",
          "metricName": "success_rate",
          "metricValue": 0.4496117154618499,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.531Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k2s03fflcy4x1ldk8k5",
          "metricName": "overall_score",
          "metricValue": 70.80892009314148,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.228Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kmc03iplcy4d7urwj07",
          "metricName": "accuracy",
          "metricValue": 0.2817199376162262,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:52.932Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13l6803lzlcy4s4e91l00",
          "metricName": "truth_score",
          "metricValue": 0.792911639479335,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.648Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lq203p9lcy46w9mmb56",
          "metricName": "accuracy",
          "metricValue": 0.548719029489373,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.362Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gthw04dulcy46ohciy6e",
          "metricName": "percentile_rank",
          "metricValue": 55.65903141375296,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:10.965Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.218Z"
    },
    {
      "id": "cmewubd9b009olcdowc7h4t70",
      "name": "together-mixtral-8x7b",
      "displayName": "Mixtral 8x7B (Together)",
      "provider": "Together",
      "description": "Powerful mixture of experts model",
      "contextWindow": 32768,
      "pricingInput": 0.0006,
      "pricingOutput": 0.0006,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "chat",
      "capabilities": "[\"chat\",\"analysis\",\"writing\",\"reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.499491540546749,
      "successRate": 0.9841032991445013,
      "passRate": 0.7020453533890841,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewubnc800cslcdos9y4lx4j",
          "metricName": "success_rate",
          "metricValue": 0.9841032991445013,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:00:12.393Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubnrf00eilcdo5vsndc3j",
          "metricName": "pass_rate",
          "metricValue": 0.7020453533890841,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:12.939Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewubqh100s6lcdou1pnnj9z",
          "metricName": "success_rate",
          "metricValue": 0.7797967343292251,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:00:16.453Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubqgm00s4lcdo0cfhb0s9",
          "metricName": "avg_response_time",
          "metricValue": 1.499491540546749,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:00:16.438Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewubo3u00g8lcdogqlwzo22",
          "metricName": "pass_rate",
          "metricValue": 0.6479996913469263,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:00:13.387Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt4604b8lcy4928dz5vs",
          "metricName": "elo_rating",
          "metricValue": 1586.474806201001,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.470Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewubog500hylcdo9trjblwv",
          "metricName": "accuracy",
          "metricValue": 0.917419110184926,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:00:13.829Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewubosg00jolcdoqzu7gps9",
          "metricName": "accuracy",
          "metricValue": 0.9257411161962776,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:00:14.272Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubp4m00lelcdo9zwlmdtl",
          "metricName": "accuracy",
          "metricValue": 0.6218695931502822,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:00:14.711Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewubpg300n4lcdoqf5h1z4t",
          "metricName": "accuracy",
          "metricValue": 0.8294440661735796,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:00:15.123Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewubpsm00oulcdo0q7d62uf",
          "metricName": "overall_score",
          "metricValue": 7.878621434315102,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:00:15.574Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewubqu800u0lcdo4qfcg6i1",
          "metricName": "average_score",
          "metricValue": 67.18613022189044,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:00:16.928Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewubr7000vqlcdopemoa5qg",
          "metricName": "win_rate",
          "metricValue": 0.4407413021783486,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:00:17.389Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j3v039jlcy45qzrdiyl",
          "metricName": "accuracy",
          "metricValue": 0.6353503065688881,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:50.971Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jne03ctlcy4zp60k8ft",
          "metricName": "success_rate",
          "metricValue": 0.4572592209239533,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.674Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k7003g3lcy4yacbt5zm",
          "metricName": "overall_score",
          "metricValue": 62.24748052215358,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.380Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kqe03jdlcy46zidz3ph",
          "metricName": "accuracy",
          "metricValue": 0.6178583408323164,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.079Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13la903mnlcy4ujuqauvd",
          "metricName": "truth_score",
          "metricValue": 0.5372022650882882,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.793Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13ltw03pxlcy44mzlwvll",
          "metricName": "accuracy",
          "metricValue": 0.3209313240185113,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.500Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtl604eilcy4xcq31xm9",
          "metricName": "percentile_rank",
          "metricValue": 39.33748446523689,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.082Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T12:59:59.328Z"
    },
    {
      "id": "cmewvelyj000nlcp4x79mfy97",
      "name": "claude-3.7-sonnet",
      "displayName": "claude-3.7-sonnet",
      "provider": "Anthropic",
      "description": "Модель для программирования от Anthropic",
      "contextWindow": 200000,
      "pricingInput": 0.003,
      "pricingOutput": 0.015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"tool_use\",\"function_calling\",\"multi_step_reasoning\",\"code_generation\",\"analysis\",\"self_reflection\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.189690006290801,
      "successRate": 0.7258056701495664,
      "passRate": 0.692924811951876,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.782,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlhb1004hlchsqm3xcs58",
          "metricName": "success_rate",
          "metricValue": 0.7258056701495664,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.749Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhre007rlchsf7drsus9",
          "metricName": "pass_rate",
          "metricValue": 0.692924811951876,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.338Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlm0m00xrlchs9xq1k981",
          "metricName": "success_rate",
          "metricValue": 0.7735139411365021,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.855Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlm0b00xplchs4yk4alm0",
          "metricName": "avg_response_time",
          "metricValue": 3.189690006290801,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.843Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli9300b1lchsog3vw4va",
          "metricName": "pass_rate",
          "metricValue": 0.6598391058189683,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.975Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtbm04colcy43p5iu0ri",
          "metricName": "elo_rating",
          "metricValue": 1570.793464541733,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.738Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlisx00eblchswwz0w1uh",
          "metricName": "accuracy",
          "metricValue": 0.8457079829296198,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.690Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljc200hllchsocpjxvid",
          "metricName": "accuracy",
          "metricValue": 0.5665011679206429,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.378Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljvc00kvlchszms5uz55",
          "metricName": "accuracy",
          "metricValue": 0.6005059403541786,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.073Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkfu00o5lchs6oflwvev",
          "metricName": "accuracy",
          "metricValue": 0.634912708625037,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.810Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkz500rflchssmz4b12r",
          "metricName": "overall_score",
          "metricValue": 7.887689515186969,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.505Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmm30119lchsvic847e7",
          "metricName": "average_score",
          "metricValue": 69.4820936662733,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.627Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln5m014jlchshr4lbrmw",
          "metricName": "win_rate",
          "metricValue": 0.6712639671260192,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.330Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjn30006lcj0hjxsy1d8",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.782,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.191Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13jc603azlcy4bip090hu",
          "metricName": "accuracy",
          "metricValue": 0.7142272768968153,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.270Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jvy03e9lcy4uwj0nzvm",
          "metricName": "success_rate",
          "metricValue": 0.3634778324060848,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.983Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kfk03hjlcy4x86l7nzp",
          "metricName": "overall_score",
          "metricValue": 75.25940439468485,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.688Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kz303ktlcy43u5mlvsi",
          "metricName": "accuracy",
          "metricValue": 0.562545773260303,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.392Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lja03o3lcy4kboik0nh",
          "metricName": "truth_score",
          "metricValue": 0.8436984834597554,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.119Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m4p03rdlcy41ko5p81j",
          "metricName": "accuracy",
          "metricValue": 0.4904224672288642,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.889Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e11f0008lc1oc3x1xen4",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.782,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.772Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtsf04fylcy4h7knjboa",
          "metricName": "percentile_rank",
          "metricValue": 27.50783509567922,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.343Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.187Z"
    },
    {
      "id": "cmewvelxb000ilcp4b21jdma5",
      "name": "claude-4-opus",
      "displayName": "claude-4-opus",
      "provider": "Anthropic",
      "description": "Модель для программирования от Anthropic (с reasoning)",
      "contextWindow": 200000,
      "pricingInput": 0.015,
      "pricingOutput": 0.075,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.607424852533707,
      "successRate": 0.8650661880891475,
      "passRate": 0.7216776530227136,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.851,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh9p0047lchs29iap89s",
          "metricName": "success_rate",
          "metricValue": 0.8650661880891475,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.701Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhq4007hlchsswd4kzws",
          "metricName": "pass_rate",
          "metricValue": 0.7216776530227136,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.293Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllx800x7lchs8yare76s",
          "metricName": "success_rate",
          "metricValue": 0.7784631234162159,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.733Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllwx00x5lchsay18mvs4",
          "metricName": "avg_response_time",
          "metricValue": 2.607424852533707,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.721Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli7900arlchstdcvmw3o",
          "metricName": "pass_rate",
          "metricValue": 0.7528796077593196,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.909Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gta204celcy48ic3bjmv",
          "metricName": "elo_rating",
          "metricValue": 838.825157545739,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.682Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlir700e1lchso984hovg",
          "metricName": "accuracy",
          "metricValue": 0.7853502925897204,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.628Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljae00hblchshkh0tv6v",
          "metricName": "accuracy",
          "metricValue": 0.5510181068115005,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.319Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljts00kllchs93nzyedc",
          "metricName": "accuracy",
          "metricValue": 0.9575907836298507,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.017Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlke800nvlchs2m5ygngz",
          "metricName": "accuracy",
          "metricValue": 0.6027116024995104,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.752Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkxk00r5lchsojckopde",
          "metricName": "overall_score",
          "metricValue": 7.367587163123113,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.448Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmkh010zlchsybgb20vp",
          "metricName": "average_score",
          "metricValue": 67.22363084409679,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.569Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln3y0149lchs1l3dpsom",
          "metricName": "win_rate",
          "metricValue": 0.7196906791003176,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.270Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjnb0008lcj0b8610203",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.851,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.200Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13jaj03aplcy4a8hyr5s4",
          "metricName": "accuracy",
          "metricValue": 0.4695898054401426,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.212Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jub03dzlcy4r8zmynk6",
          "metricName": "success_rate",
          "metricValue": 0.4215162034056921,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.923Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kdx03h9lcy4yuc8ifu7",
          "metricName": "overall_score",
          "metricValue": 58.75789436024958,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.629Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kxf03kjlcy4qsddo8ls",
          "metricName": "accuracy",
          "metricValue": 0.4253737668211012,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.331Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lhl03ntlcy4r48e8sf6",
          "metricName": "truth_score",
          "metricValue": 0.6650693934334628,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.057Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m2z03r3lcy4noquinjk",
          "metricName": "accuracy",
          "metricValue": 0.5308404379093936,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.828Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e11o000alc1oafmbbly5",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.851,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.781Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtr104folcy4cr1wk1jm",
          "metricName": "percentile_rank",
          "metricValue": 74.62209216307403,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.293Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.143Z"
    },
    {
      "id": "cmewvelsh0000lcp4a943r8qh",
      "name": "claude-4-sonnet",
      "displayName": "claude-4-sonnet",
      "provider": "Anthropic",
      "description": "Модель для программирования от Anthropic",
      "contextWindow": 200000,
      "pricingInput": 0.003,
      "pricingOutput": 0.015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.211398999113937,
      "successRate": 0.8880908372427774,
      "passRate": 0.9030702853761645,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.8270000000000001,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh4j0037lchsjev7gk5u",
          "metricName": "success_rate",
          "metricValue": 0.8880908372427774,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.516Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhkx006hlchsgoxofhr1",
          "metricName": "pass_rate",
          "metricValue": 0.9030702853761645,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.106Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlllf00v7lchseft73wql",
          "metricName": "success_rate",
          "metricValue": 0.7460394837134511,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.307Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlll400v5lchsfwdge5ja",
          "metricName": "avg_response_time",
          "metricValue": 1.211398999113937,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.296Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli17009rlchsfgu301sq",
          "metricName": "pass_rate",
          "metricValue": 0.7392407648574595,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.692Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt5104belcy407r0fcmr",
          "metricName": "elo_rating",
          "metricValue": 1656.010630006635,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.502Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlil400d1lchshk97omwq",
          "metricName": "accuracy",
          "metricValue": 0.7407499995837776,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.409Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj4i00gblchs8qct8nv8",
          "metricName": "accuracy",
          "metricValue": 0.9264031619373637,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.106Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljnt00jllchsshwdtqal",
          "metricName": "accuracy",
          "metricValue": 0.7039059493055537,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.801Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk7b00mvlchs7wb1qfd3",
          "metricName": "accuracy",
          "metricValue": 0.8965231900898185,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.504Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkrf00q5lchsdht1rim0",
          "metricName": "overall_score",
          "metricValue": 8.164341183005472,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.228Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlme500zzlchsuqpfml1w",
          "metricName": "average_score",
          "metricValue": 69.17681592551094,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.341Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvlmxy0139lchsb19hpuas",
          "metricName": "win_rate",
          "metricValue": 0.7393743968408182,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.055Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjnj000alcj00p1lydtn",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.8270000000000001,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.208Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j4s039plcy4xim2seuw",
          "metricName": "accuracy",
          "metricValue": 0.4298912332403407,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.004Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13joc03czlcy4g3pyzy77",
          "metricName": "success_rate",
          "metricValue": 0.5854308608695986,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.708Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k7z03g9lcy4neyn4mwl",
          "metricName": "overall_score",
          "metricValue": 69.29868201199962,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.415Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13krj03jjlcy4iy2jkm1l",
          "metricName": "accuracy",
          "metricValue": 0.6337681982693224,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.119Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lbd03mtlcy4oiudubo5",
          "metricName": "truth_score",
          "metricValue": 0.8919069773616219,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.833Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13luu03q3lcy4rs99no09",
          "metricName": "accuracy",
          "metricValue": 0.4299209233951598,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.534Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e11y000clc1ofvfitqcr",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.8270000000000001,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.790Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtm304eolcy4tb0ux3s8",
          "metricName": "percentile_rank",
          "metricValue": 77.7808528599165,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.115Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:29.970Z"
    },
    {
      "id": "cmewvelsv0001lcp4xxkkf8y6",
      "name": "claude-4-sonnet-1m",
      "displayName": "claude-4-sonnet-1m",
      "provider": "Anthropic",
      "description": "Модель для программирования от Anthropic",
      "contextWindow": 1000000,
      "pricingInput": 0.003,
      "pricingOutput": 0.015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.81338670922867,
      "successRate": 0.9338969694738433,
      "passRate": 0.7127485121221289,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.815,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh4v0039lchsbtx3j0hj",
          "metricName": "success_rate",
          "metricValue": 0.9338969694738433,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.528Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhl7006jlchslnmit7wm",
          "metricName": "pass_rate",
          "metricValue": 0.7127485121221289,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.116Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllm400vblchsenrmhdv2",
          "metricName": "success_rate",
          "metricValue": 0.9426783050609738,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.332Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlllr00v9lchsupd386sy",
          "metricName": "avg_response_time",
          "metricValue": 3.81338670922867,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.320Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli1k009tlchsv6la2z2p",
          "metricName": "pass_rate",
          "metricValue": 0.6737549370346444,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.705Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt5d04bglcy4bftm7z5l",
          "metricName": "elo_rating",
          "metricValue": 1281.075423254011,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.514Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlilh00d3lchsvii4ick4",
          "metricName": "accuracy",
          "metricValue": 0.8892389783848933,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.421Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj4v00gdlchsed1z5okd",
          "metricName": "accuracy",
          "metricValue": 0.8905748270844998,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.119Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljo400jnlchslr5dlwpb",
          "metricName": "accuracy",
          "metricValue": 0.7572835034746328,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.812Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk7o00mxlchsqgg9pm6l",
          "metricName": "accuracy",
          "metricValue": 0.8686267150795857,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.516Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkrr00q7lchsbvih7u1i",
          "metricName": "overall_score",
          "metricValue": 8.425579650180055,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.240Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmeg0101lchso3r6m0mx",
          "metricName": "average_score",
          "metricValue": 74.39452310194116,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.352Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvlmyd013blchs340v07yr",
          "metricName": "win_rate",
          "metricValue": 0.539315131708987,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.069Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex0zw2x000slcd448p1tyol",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.815,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T16:07:01.161Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j53039rlcy4tgwvmds0",
          "metricName": "accuracy",
          "metricValue": 0.71644656945819,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.016Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13joo03d1lcy4nhqxy70t",
          "metricName": "success_rate",
          "metricValue": 0.4924454077079136,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.720Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k8b03gblcy4uu0c944m",
          "metricName": "overall_score",
          "metricValue": 76.57816634351357,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.427Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13krv03jllcy45w40nubi",
          "metricName": "accuracy",
          "metricValue": 0.6621026563040648,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.131Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lbp03mvlcy4r4j8pepp",
          "metricName": "truth_score",
          "metricValue": 0.5466362988849038,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.845Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lv703q5lcy4u8n4dbfg",
          "metricName": "accuracy",
          "metricValue": 0.4363723991995981,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.547Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e143000ulc1oyambd9o6",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.815,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.867Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtmc04eqlcy4oj76h5mi",
          "metricName": "percentile_rank",
          "metricValue": 15.00915092252738,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.124Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:29.984Z"
    },
    {
      "id": "cmewvelx1000hlcp4s2jxns3v",
      "name": "claude-4.1-opus",
      "displayName": "claude-4.1-opus",
      "provider": "Anthropic",
      "description": "Модель для программирования от Anthropic (с reasoning)",
      "contextWindow": 200000,
      "pricingInput": 0.015,
      "pricingOutput": 0.075,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.528472055679859,
      "successRate": 0.9555915468690843,
      "passRate": 0.7459921370710667,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh9g0045lchsx24n9sam",
          "metricName": "success_rate",
          "metricValue": 0.9555915468690843,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.692Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhpu007flchs0iim1i63",
          "metricName": "pass_rate",
          "metricValue": 0.7459921370710667,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.282Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllwm00x3lchs3sfigiej",
          "metricName": "success_rate",
          "metricValue": 0.7475239851978908,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.710Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllwb00x1lchsr6gma8up",
          "metricName": "avg_response_time",
          "metricValue": 1.528472055679859,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.700Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli6v00aplchsovc8bdrz",
          "metricName": "pass_rate",
          "metricValue": 0.6429083672325466,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.895Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt9t04cclcy488dtefde",
          "metricName": "elo_rating",
          "metricValue": 1518.646118397614,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.673Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvliqu00dzlchsy66bi8v3",
          "metricName": "accuracy",
          "metricValue": 0.793686610624662,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.614Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlja300h9lchss3z59go4",
          "metricName": "accuracy",
          "metricValue": 0.661348474535135,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.307Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljtg00kjlchs17h118br",
          "metricName": "accuracy",
          "metricValue": 0.6665562106710361,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.005Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkdw00ntlchsk97e6h1d",
          "metricName": "accuracy",
          "metricValue": 0.9857533125904262,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.740Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkx800r3lchsqcqzt3wz",
          "metricName": "overall_score",
          "metricValue": 8.307876046731371,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.436Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmk3010xlchs3r5q3mh1",
          "metricName": "average_score",
          "metricValue": 72.05747980737092,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.555Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln3n0147lchsp7wztog0",
          "metricName": "win_rate",
          "metricValue": 0.4457917127238465,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.259Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ja703anlcy42oot6iwn",
          "metricName": "accuracy",
          "metricValue": 0.7243097214704268,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.199Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13ju003dxlcy43a9adxzm",
          "metricName": "success_rate",
          "metricValue": 0.4041580055320547,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.912Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kdl03h7lcy44k2acu22",
          "metricName": "overall_score",
          "metricValue": 64.94370021015908,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.617Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kx303khlcy42ap5cz9n",
          "metricName": "accuracy",
          "metricValue": 0.4953665068397095,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.319Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lha03nrlcy4h42sm5pf",
          "metricName": "truth_score",
          "metricValue": 0.7633153929170584,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.046Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m2n03r1lcy4bwwm0n7p",
          "metricName": "accuracy",
          "metricValue": 0.5541239888295248,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.815Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtqr04fmlcy4eekduijt",
          "metricName": "percentile_rank",
          "metricValue": 50.32237086742138,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.283Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.134Z"
    },
    {
      "id": "cmewvelvs000clcp4ls80hqbs",
      "name": "deepseek-r1-0528",
      "displayName": "deepseek-r1-0528",
      "provider": "DeepSeek",
      "description": "Модель для программирования от DeepSeek (бесплатная)",
      "contextWindow": 32768,
      "pricingInput": 0.00014,
      "pricingOutput": 0.00028,
      "isFree": true,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.777308334972741,
      "successRate": 0.85863373198166,
      "passRate": 0.6137640584412465,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.6890000000000001,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh83003vlchs05rp4x1t",
          "metricName": "success_rate",
          "metricValue": 0.85863373198166,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.644Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhod0075lchsx6y9ll85",
          "metricName": "pass_rate",
          "metricValue": 0.6137640584412465,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.230Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlltf00wjlchs5aobf7oz",
          "metricName": "success_rate",
          "metricValue": 0.8244260467092654,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.596Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllt200whlchsj140yals",
          "metricName": "avg_response_time",
          "metricValue": 3.777308334972741,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.583Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli5a00aflchs4qv3zmu0",
          "metricName": "pass_rate",
          "metricValue": 0.7461934174026404,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.838Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt8g04c2lcy4tlr5gs3z",
          "metricName": "elo_rating",
          "metricValue": 1599.589656720525,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.624Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlip700dplchs9fkhj7n6",
          "metricName": "accuracy",
          "metricValue": 0.9373091965449123,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.556Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj8i00gzlchs6y0go1bh",
          "metricName": "accuracy",
          "metricValue": 0.5511245056734437,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.251Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljrr00k9lchsr8hyxgrg",
          "metricName": "accuracy",
          "metricValue": 0.7211026242121661,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.943Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkb700njlchs7reznh5t",
          "metricName": "accuracy",
          "metricValue": 0.9022134366569922,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.644Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkvh00qtlchsr7728urn",
          "metricName": "overall_score",
          "metricValue": 7.058535756108319,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.373Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmic010nlchsb0ox8oon",
          "metricName": "average_score",
          "metricValue": 77.31039262150863,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.492Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln1y013xlchsc7z2t59y",
          "metricName": "win_rate",
          "metricValue": 0.7550169160172296,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.198Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjoj000ilcj0vk2l82y7",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.6890000000000001,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.243Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j8j03adlcy4nmqtdcth",
          "metricName": "accuracy",
          "metricValue": 0.4137081154742457,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.139Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jse03dnlcy4d3mkjli9",
          "metricName": "success_rate",
          "metricValue": 0.4768650490495803,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.854Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kbx03gxlcy4iuooprxc",
          "metricName": "overall_score",
          "metricValue": 59.07792198260732,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.557Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kvf03k7lcy4wq6etf2o",
          "metricName": "accuracy",
          "metricValue": 0.579451160678387,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.260Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lfc03nhlcy4sbrahgfl",
          "metricName": "truth_score",
          "metricValue": 0.5773703642137241,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.976Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m1003qrlcy44imhpls1",
          "metricName": "accuracy",
          "metricValue": 0.3548401127850659,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.756Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e12u000klc1ovwjhb4l4",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.6890000000000001,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.823Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtpb04fclcy4cp4dj2kx",
          "metricName": "percentile_rank",
          "metricValue": 65.83789949412787,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.231Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.088Z"
    },
    {
      "id": "cmewvelw0000dlcp4hcii0i4n",
      "name": "deepseek-v3.1",
      "displayName": "deepseek-v3.1",
      "provider": "DeepSeek",
      "description": "Модель для программирования от DeepSeek (бесплатная) (с reasoning)",
      "contextWindow": 32768,
      "pricingInput": 0.00014,
      "pricingOutput": 0.00028,
      "isFree": true,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.67298419117812,
      "successRate": 0.8817308424347906,
      "passRate": 0.795341118425241,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.72,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh8c003xlchsad4ze95l",
          "metricName": "success_rate",
          "metricValue": 0.8817308424347906,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.652Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhom0077lchs8773sw95",
          "metricName": "pass_rate",
          "metricValue": 0.795341118425241,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.239Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllu300wnlchszlxcuqeo",
          "metricName": "success_rate",
          "metricValue": 0.990346967993634,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.620Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllts00wllchsto0bj0o5",
          "metricName": "avg_response_time",
          "metricValue": 3.67298419117812,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.608Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli5l00ahlchsbh8k5air",
          "metricName": "pass_rate",
          "metricValue": 0.5593781916041284,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.850Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt8q04c4lcy49g4816l6",
          "metricName": "elo_rating",
          "metricValue": 1052.736418131387,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.635Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlipk00drlchs0t93ubvs",
          "metricName": "accuracy",
          "metricValue": 0.783820061816478,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.568Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj8u00h1lchsnj89tonf",
          "metricName": "accuracy",
          "metricValue": 0.7472681032329664,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.262Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljs400kblchsd5k5ebed",
          "metricName": "accuracy",
          "metricValue": 0.6197092880092355,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.956Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkbl00nllchsfpwxzori",
          "metricName": "accuracy",
          "metricValue": 0.6979006062474739,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.657Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkvt00qvlchsiag0hf7w",
          "metricName": "overall_score",
          "metricValue": 8.895253415201044,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.385Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmio010plchs4aemwcip",
          "metricName": "average_score",
          "metricValue": 61.16029902663946,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.504Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln29013zlchstgnbi2rp",
          "metricName": "win_rate",
          "metricValue": 0.5078773682580531,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.209Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjos000klcj0jnr66cwd",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.72,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.252Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j8u03aflcy44j20j60n",
          "metricName": "accuracy",
          "metricValue": 0.7846214454700411,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.150Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jsp03dplcy42n3680hr",
          "metricName": "success_rate",
          "metricValue": 0.492640994593931,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.866Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kc703gzlcy4v0e1trj1",
          "metricName": "overall_score",
          "metricValue": 60.1355263840554,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.568Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kvs03k9lcy42wmlxhvp",
          "metricName": "accuracy",
          "metricValue": 0.3407198282350776,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.272Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lfp03njlcy4c1jtt0p6",
          "metricName": "truth_score",
          "metricValue": 0.5144916927877212,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.989Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m1c03qtlcy42xlg4wad",
          "metricName": "accuracy",
          "metricValue": 0.3932976012981119,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.768Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e132000mlc1obfaw2en9",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.72,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.830Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtpl04felcy4sulaa4k2",
          "metricName": "percentile_rank",
          "metricValue": 16.17185425285769,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.241Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.096Z"
    },
    {
      "id": "cmewvely1000llcp4j7w2x7hm",
      "name": "gemini-2.5-flash",
      "displayName": "gemini-2.5-flash",
      "provider": "Google",
      "description": "Модель для программирования от Google (с reasoning)",
      "contextWindow": 1048576,
      "pricingInput": 0.000075,
      "pricingOutput": 0.0003,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"tool_use\",\"function_calling\",\"multi_step_reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.99061924526402,
      "successRate": 0.8345828907305595,
      "passRate": 0.9863588995781497,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.764,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlhah004dlchscwn639i2",
          "metricName": "success_rate",
          "metricValue": 0.8345828907305595,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.729Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhqx007nlchshgy7u2rx",
          "metricName": "pass_rate",
          "metricValue": 0.9863588995781497,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.321Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllz900xjlchsv1xje5zm",
          "metricName": "success_rate",
          "metricValue": 0.7859377897702357,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.806Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllyx00xhlchso6qzoke0",
          "metricName": "avg_response_time",
          "metricValue": 2.99061924526402,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.793Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli8d00axlchsrn5osawj",
          "metricName": "pass_rate",
          "metricValue": 0.8239347856432333,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.950Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtax04cklcy4e8unl9g7",
          "metricName": "elo_rating",
          "metricValue": 1757.489869690301,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.713Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlis800e7lchs94l1zwqz",
          "metricName": "accuracy",
          "metricValue": 0.9490544589110566,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.664Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljbe00hhlchs9fy7mn8u",
          "metricName": "accuracy",
          "metricValue": 0.8661312474891094,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.355Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljus00krlchspd91uq2o",
          "metricName": "accuracy",
          "metricValue": 0.9545447378944328,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.052Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkf700o1lchsfrsg9u7o",
          "metricName": "accuracy",
          "metricValue": 0.7731524861347665,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.788Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkyj00rblchsy4tman7p",
          "metricName": "overall_score",
          "metricValue": 7.596586712484336,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.483Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmlf0115lchsut2o5hih",
          "metricName": "average_score",
          "metricValue": 62.55604109745757,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.604Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln4y014flchs6lo5cnfl",
          "metricName": "win_rate",
          "metricValue": 0.727155898795376,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.306Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjo9000glcj07gt10q86",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.764,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.233Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13jbi03avlcy4n9j99xhd",
          "metricName": "accuracy",
          "metricValue": 0.7560081302273033,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.247Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jv903e5lcy401j7ogg9",
          "metricName": "success_rate",
          "metricValue": 0.4398393850576425,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.958Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kev03hflcy47nr3c9h7",
          "metricName": "overall_score",
          "metricValue": 61.63563223009027,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.663Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kyf03kplcy4c682ugpc",
          "metricName": "accuracy",
          "metricValue": 0.4439726476171905,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.367Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lin03nzlcy4u56zrg3u",
          "metricName": "truth_score",
          "metricValue": 0.5326523549533386,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.095Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m3y03r9lcy4dgdt41xr",
          "metricName": "accuracy",
          "metricValue": 0.3598648045890735,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.862Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e12e000glc1oyn6qzg64",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.764,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.806Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtrw04fulcy4zhkegozd",
          "metricName": "percentile_rank",
          "metricValue": 64.99897988300934,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.324Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.170Z"
    },
    {
      "id": "cmewvelv00009lcp4iyjen994",
      "name": "gemini-2.5-pro",
      "displayName": "gemini-2.5-pro",
      "provider": "Google",
      "description": "Модель для программирования от Google (с reasoning)",
      "contextWindow": 2097152,
      "pricingInput": 0.00025,
      "pricingOutput": 0.0005,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"tool_use\",\"function_calling\",\"multi_step_reasoning\",\"code_generation\",\"analysis\",\"vision\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.799329601357594,
      "successRate": 0.9517438548240016,
      "passRate": 0.7441158431208774,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.831,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh79003plchsrnj7cdx4",
          "metricName": "success_rate",
          "metricValue": 0.9517438548240016,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.614Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhne006zlchsd7tnbtwp",
          "metricName": "pass_rate",
          "metricValue": 0.7441158431208774,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.194Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllre00w7lchsz3xu9b9l",
          "metricName": "success_rate",
          "metricValue": 0.887480382861106,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.522Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllr200w5lchsi9u4sjow",
          "metricName": "avg_response_time",
          "metricValue": 2.799329601357594,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.510Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli4c00a9lchseel3h678",
          "metricName": "pass_rate",
          "metricValue": 0.7528041586189724,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.804Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt7o04bwlcy4kbcykxps",
          "metricName": "elo_rating",
          "metricValue": 846.2691608911683,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.596Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlio600djlchsxdx771xn",
          "metricName": "accuracy",
          "metricValue": 0.9705565677064393,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.518Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj7j00gtlchs5al3xfkl",
          "metricName": "accuracy",
          "metricValue": 0.5894772596616442,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.215Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljqn00k3lchssy4pio8b",
          "metricName": "accuracy",
          "metricValue": 0.8004667638695633,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.904Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlka900ndlchsgwk8cc4l",
          "metricName": "accuracy",
          "metricValue": 0.9625865778211693,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.610Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkuj00qnlchsapih40kj",
          "metricName": "overall_score",
          "metricValue": 9.45902192643198,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.339Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmgz010hlchs881qjcpa",
          "metricName": "average_score",
          "metricValue": 62.3141829696327,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.444Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln11013rlchszdl149tx",
          "metricName": "win_rate",
          "metricValue": 0.6937732981257088,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.166Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjo0000elcj0nvo8w6yd",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.831,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.225Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j7n03a7lcy4x8xh4z2x",
          "metricName": "accuracy",
          "metricValue": 0.4249913197599642,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.107Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jrd03dhlcy4i63bmmof",
          "metricName": "success_rate",
          "metricValue": 0.5474632572812186,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.817Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kaw03grlcy4qhqj75k7",
          "metricName": "overall_score",
          "metricValue": 50.18649097886825,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.520Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kuf03k1lcy4qxojrovt",
          "metricName": "accuracy",
          "metricValue": 0.6620728533537934,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.223Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lef03nblcy4arw7vzyk",
          "metricName": "truth_score",
          "metricValue": 0.8430541702141361,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.943Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lzt03qllcy4pd4509p4",
          "metricName": "accuracy",
          "metricValue": 0.4845058611008748,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.713Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e126000elc1or3it7c1f",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.831,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.798Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtof04f6lcy46f9k8ws5",
          "metricName": "percentile_rank",
          "metricValue": 77.14869343382884,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.200Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.061Z"
    },
    {
      "id": "cmewvelyb000mlcp47tik9ptm",
      "name": "gpt-4.1",
      "displayName": "gpt-4.1",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI",
      "contextWindow": 128000,
      "pricingInput": 0.01,
      "pricingOutput": 0.03,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.295695605976147,
      "successRate": 0.8962669092827589,
      "passRate": 0.8211650836172428,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlhar004flchsf8d98vtb",
          "metricName": "success_rate",
          "metricValue": 0.8962669092827589,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.739Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhr5007plchspeaod260",
          "metricName": "pass_rate",
          "metricValue": 0.8211650836172428,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.329Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllzy00xnlchs1rr1tznk",
          "metricName": "success_rate",
          "metricValue": 0.94889002022809,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.830Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllzl00xllchss12xiav8",
          "metricName": "avg_response_time",
          "metricValue": 1.295695605976147,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.818Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli8r00azlchsdach5tpb",
          "metricName": "pass_rate",
          "metricValue": 0.7228375350441049,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.963Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtba04cmlcy4eqr7p07z",
          "metricName": "elo_rating",
          "metricValue": 1476.126568231288,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.726Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlisk00e9lchsgwgabw75",
          "metricName": "accuracy",
          "metricValue": 0.8223352325537729,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.676Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljbp00hjlchs23h46w71",
          "metricName": "accuracy",
          "metricValue": 0.7344158869509756,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.366Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljv300ktlchsyaakn14d",
          "metricName": "accuracy",
          "metricValue": 0.8733260768988496,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.063Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkfj00o3lchsvb7bue5u",
          "metricName": "accuracy",
          "metricValue": 0.9247743684621159,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.799Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkyu00rdlchszb8rms5y",
          "metricName": "overall_score",
          "metricValue": 7.224183470588477,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.494Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmlr0117lchs9pm2hid5",
          "metricName": "average_score",
          "metricValue": 60.49542504316882,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.615Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln5b014hlchsem5yg3mr",
          "metricName": "win_rate",
          "metricValue": 0.547099922512728,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.319Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13jbt03axlcy439la77n0",
          "metricName": "accuracy",
          "metricValue": 0.5362294524056466,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.258Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jvm03e7lcy4iepqzuj7",
          "metricName": "success_rate",
          "metricValue": 0.3591736871559865,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.970Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kf603hhlcy406xcg6cc",
          "metricName": "overall_score",
          "metricValue": 55.72411792589347,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.674Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kys03krlcy4njz3tc9j",
          "metricName": "accuracy",
          "metricValue": 0.3236733157136462,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.380Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13liz03o1lcy412li2gp4",
          "metricName": "truth_score",
          "metricValue": 0.5328826733872453,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.107Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m4a03rblcy4x9fg3n1k",
          "metricName": "accuracy",
          "metricValue": 0.3369508926135274,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.874Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gts504fwlcy4fmsob7gs",
          "metricName": "percentile_rank",
          "metricValue": 23.19726693506876,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.333Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.179Z"
    },
    {
      "id": "cmewvelt70002lcp4k5l3oi2w",
      "name": "gpt-5",
      "displayName": "gpt-5",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI",
      "contextWindow": 128000,
      "pricingInput": 0.005,
      "pricingOutput": 0.015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.979028254841732,
      "successRate": 0.9645061844708067,
      "passRate": 0.6498121753584452,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.792,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh57003blchsijble2pw",
          "metricName": "success_rate",
          "metricValue": 0.9645061844708067,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.539Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhli006llchspbgnrcml",
          "metricName": "pass_rate",
          "metricValue": 0.6498121753584452,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.126Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllms00vflchsq5s15kzo",
          "metricName": "success_rate",
          "metricValue": 0.9860348528293108,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.356Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllmg00vdlchswfx6617v",
          "metricName": "avg_response_time",
          "metricValue": 1.979028254841732,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.344Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli1w009vlchssjjath6f",
          "metricName": "pass_rate",
          "metricValue": 0.5650193561121806,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.716Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt5o04bilcy44iwpc9xm",
          "metricName": "elo_rating",
          "metricValue": 1427.185227657407,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.524Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlils00d5lchsscc6xb95",
          "metricName": "accuracy",
          "metricValue": 0.8356485067447598,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.432Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj5600gflchs73k655g3",
          "metricName": "accuracy",
          "metricValue": 0.8505777277221159,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.130Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljof00jplchsaeijjneo",
          "metricName": "accuracy",
          "metricValue": 0.6689440074729275,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.823Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk8000mzlchszs3d1a0q",
          "metricName": "accuracy",
          "metricValue": 0.6195932114712115,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.528Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlks300q9lchsoc0u7r82",
          "metricName": "overall_score",
          "metricValue": 9.440888135556754,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.251Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmer0103lchssan2fkr3",
          "metricName": "average_score",
          "metricValue": 63.45696793352621,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.364Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvlmyq013dlchsqmpwyajl",
          "metricName": "win_rate",
          "metricValue": 0.4027518675263249,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.083Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex0zw37000ulcd4qrq4xyla",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.792,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T16:07:01.171Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j5f039tlcy4wux2ccee",
          "metricName": "accuracy",
          "metricValue": 0.5643059381948602,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.027Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13joz03d3lcy42zny1m5h",
          "metricName": "success_rate",
          "metricValue": 0.3198250339408784,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.732Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k8n03gdlcy4yrvwqhxx",
          "metricName": "overall_score",
          "metricValue": 77.16106981095339,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.439Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ks603jnlcy40luwguzh",
          "metricName": "accuracy",
          "metricValue": 0.4991208914718685,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.142Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lc403mxlcy47q8ixull",
          "metricName": "truth_score",
          "metricValue": 0.7620148998560876,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.860Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lvk03q7lcy41h0jbc9g",
          "metricName": "accuracy",
          "metricValue": 0.4842229124479218,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.560Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e14b000wlc1ovctpnush",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.792,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.876Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtmk04eslcy4veqoe7fv",
          "metricName": "percentile_rank",
          "metricValue": 18.81872262700478,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.132Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:29.995Z"
    },
    {
      "id": "cmewvelth0003lcp41trzr4yu",
      "name": "gpt-5-fast",
      "displayName": "gpt-5-fast",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.0025,
      "pricingOutput": 0.01,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.381249117519751,
      "successRate": 0.7617390848979704,
      "passRate": 0.8739733504967515,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh5j003dlchs1adn44jb",
          "metricName": "success_rate",
          "metricValue": 0.7617390848979704,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.551Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhls006nlchskdem5tsr",
          "metricName": "pass_rate",
          "metricValue": 0.8739733504967515,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.136Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllne00vjlchs0c21j5tu",
          "metricName": "success_rate",
          "metricValue": 0.9311837036014106,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.378Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlln200vhlchsprozdvj9",
          "metricName": "avg_response_time",
          "metricValue": 1.381249117519751,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.366Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli28009xlchs47bqce0m",
          "metricName": "pass_rate",
          "metricValue": 0.5339696898530624,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.729Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt5y04bklcy4uh2wkhbp",
          "metricName": "elo_rating",
          "metricValue": 1333.591665658894,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.534Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlimb00d7lchs3z7jt94u",
          "metricName": "accuracy",
          "metricValue": 0.819644608867546,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.451Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj5i00ghlchssykac84n",
          "metricName": "accuracy",
          "metricValue": 0.6229066218462905,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.142Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljor00jrlchsliqmr5ge",
          "metricName": "accuracy",
          "metricValue": 0.9299964637481557,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.835Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk8d00n1lchst45wpd7q",
          "metricName": "accuracy",
          "metricValue": 0.6432499845692458,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.541Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkse00qblchslgetbrtj",
          "metricName": "overall_score",
          "metricValue": 6.119235194181351,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.263Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmf60105lchszew7wbs9",
          "metricName": "average_score",
          "metricValue": 63.15304531592349,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.378Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvlmz4013flchs376p0hi7",
          "metricName": "win_rate",
          "metricValue": 0.5966046514758775,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.096Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j5r039vlcy40yoajqob",
          "metricName": "accuracy",
          "metricValue": 0.7783470130466642,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.039Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jpc03d5lcy41q9e4oxf",
          "metricName": "success_rate",
          "metricValue": 0.4415533542382383,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.744Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k8y03gflcy4gzosb33w",
          "metricName": "overall_score",
          "metricValue": 50.34901254247188,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.450Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ksh03jplcy4xt8r805q",
          "metricName": "accuracy",
          "metricValue": 0.6816586890362977,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.153Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lcg03mzlcy4fr4i4qbc",
          "metricName": "truth_score",
          "metricValue": 0.8802140480182135,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.872Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lvw03q9lcy4evldn9l2",
          "metricName": "accuracy",
          "metricValue": 0.4179582900152759,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.572Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtms04eulcy4rd60fg67",
          "metricName": "percentile_rank",
          "metricValue": 16.93200073889447,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.141Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.005Z"
    },
    {
      "id": "cmewvelts0004lcp4pguo4zkr",
      "name": "gpt-5-high",
      "displayName": "gpt-5-high",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.01,
      "pricingOutput": 0.03,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.672864796781704,
      "successRate": 0.8108981505909837,
      "passRate": 0.8040209807061807,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.821,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh5v003flchsfdjoy889",
          "metricName": "success_rate",
          "metricValue": 0.8108981505909837,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.563Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhm1006plchs77qcld49",
          "metricName": "pass_rate",
          "metricValue": 0.8040209807061807,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.145Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllo600vnlchshswguoxh",
          "metricName": "success_rate",
          "metricValue": 0.7237564386876812,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.406Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllnu00vllchs4mho5gzo",
          "metricName": "avg_response_time",
          "metricValue": 2.672864796781704,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.394Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli2o009zlchson3zf6xd",
          "metricName": "pass_rate",
          "metricValue": 0.8286618162247859,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.744Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt6904bmlcy4ffe1xibg",
          "metricName": "elo_rating",
          "metricValue": 1358.961557305898,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.546Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlimm00d9lchsg5vypdk7",
          "metricName": "accuracy",
          "metricValue": 0.8376520573854496,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.462Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj5t00gjlchsgpdaopf2",
          "metricName": "accuracy",
          "metricValue": 0.8138489186567355,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.153Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljp200jtlchslcr07f08",
          "metricName": "accuracy",
          "metricValue": 0.6218107767016497,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.847Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk8o00n3lchs4d0fq1cc",
          "metricName": "accuracy",
          "metricValue": 0.8010186383644446,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.552Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlksr00qdlchsu6yy87qy",
          "metricName": "overall_score",
          "metricValue": 9.672895963610614,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.275Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmff0107lchsb0w6rtor",
          "metricName": "average_score",
          "metricValue": 68.66722492469441,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.387Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvlmzg013hlchs2l37gany",
          "metricName": "win_rate",
          "metricValue": 0.6342934216698454,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.108Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex0zw3i000wlcd48vmhysud",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.821,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T16:07:01.182Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j62039xlcy4bnr3yz2w",
          "metricName": "accuracy",
          "metricValue": 0.7867859065304486,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.050Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jpo03d7lcy4weqm5mu4",
          "metricName": "success_rate",
          "metricValue": 0.4125171481302704,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.756Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k9a03ghlcy46atwbes1",
          "metricName": "overall_score",
          "metricValue": 73.63733588661694,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.462Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kst03jrlcy4bm9lxe7f",
          "metricName": "accuracy",
          "metricValue": 0.2468090834639425,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.166Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lcr03n1lcy4svz29hvb",
          "metricName": "truth_score",
          "metricValue": 0.5562802266660468,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.883Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lw603qblcy4oipsj590",
          "metricName": "accuracy",
          "metricValue": 0.3546950918785933,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.582Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e14j000ylc1oigdnmo3m",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.821,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.884Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtn004ewlcy4fl4xuwlb",
          "metricName": "percentile_rank",
          "metricValue": 54.53429168462188,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.149Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.016Z"
    },
    {
      "id": "cmewvelu10005lcp42wntv22h",
      "name": "gpt-5-high-fast",
      "displayName": "gpt-5-high-fast",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.005,
      "pricingOutput": 0.015,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.117234268064043,
      "successRate": 0.9686842180937059,
      "passRate": 0.8870907514501573,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh66003hlchsatfrkc2k",
          "metricName": "success_rate",
          "metricValue": 0.9686842180937059,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.574Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhma006rlchselerm6ju",
          "metricName": "pass_rate",
          "metricValue": 0.8870907514501573,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.155Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllot00vrlchsyhwclhae",
          "metricName": "success_rate",
          "metricValue": 0.9209244732594527,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.430Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlloi00vplchsky7mod4x",
          "metricName": "avg_response_time",
          "metricValue": 1.117234268064043,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.418Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli2z00a1lchshwgy3km8",
          "metricName": "pass_rate",
          "metricValue": 0.8329104867294845,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.755Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt6k04bolcy472vczums",
          "metricName": "elo_rating",
          "metricValue": 1055.212570144426,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.556Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlimy00dblchspxf5dp63",
          "metricName": "accuracy",
          "metricValue": 0.7570551450114666,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.474Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj6500gllchsod065l14",
          "metricName": "accuracy",
          "metricValue": 0.6133084072739035,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.165Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljpe00jvlchs8z3u59dr",
          "metricName": "accuracy",
          "metricValue": 0.807591670671755,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.858Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk8y00n5lchsncx2lctt",
          "metricName": "accuracy",
          "metricValue": 0.8677675684504793,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.563Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkt300qflchs9d60yalr",
          "metricName": "overall_score",
          "metricValue": 7.043978670332941,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.287Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmfp0109lchs6ldxke7f",
          "metricName": "average_score",
          "metricValue": 60.47048442310421,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.397Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvlmzr013jlchsfd1prla3",
          "metricName": "win_rate",
          "metricValue": 0.7800137530945812,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.119Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j6d039zlcy42eww6wji",
          "metricName": "accuracy",
          "metricValue": 0.755523987908328,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.062Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jq003d9lcy4r1h5fuqv",
          "metricName": "success_rate",
          "metricValue": 0.3697233834590509,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.768Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k9l03gjlcy4ffoaamhs",
          "metricName": "overall_score",
          "metricValue": 72.45303894252046,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.474Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kt403jtlcy4xksohpba",
          "metricName": "accuracy",
          "metricValue": 0.4888342519575249,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.177Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13ld303n3lcy4gl7nembw",
          "metricName": "truth_score",
          "metricValue": 0.7033848970626091,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.896Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lwi03qdlcy4y1ruzjj2",
          "metricName": "accuracy",
          "metricValue": 0.3930897715278585,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.594Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtna04eylcy4sf70r01a",
          "metricName": "percentile_rank",
          "metricValue": 73.38122036621388,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.158Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.025Z"
    },
    {
      "id": "cmewvelu90006lcp418tfy8yv",
      "name": "gpt-5-low",
      "displayName": "gpt-5-low",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.001,
      "pricingOutput": 0.002,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.991312536574185,
      "successRate": 0.74975771661001,
      "passRate": 0.7467048567755155,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh6g003jlchszzjaqvqx",
          "metricName": "success_rate",
          "metricValue": 0.74975771661001,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.584Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhmj006tlchsq6duvwh2",
          "metricName": "pass_rate",
          "metricValue": 0.7467048567755155,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.164Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllpg00vvlchs6iugkq15",
          "metricName": "success_rate",
          "metricValue": 0.9898026760057824,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.452Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllp500vtlchs8b8mfvda",
          "metricName": "avg_response_time",
          "metricValue": 2.991312536574185,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.441Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli3c00a3lchsgwi2oxum",
          "metricName": "pass_rate",
          "metricValue": 0.8366966136254297,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.769Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt6t04bqlcy44mzew1im",
          "metricName": "elo_rating",
          "metricValue": 1676.638375697915,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.565Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlin800ddlchs7gdv9ktz",
          "metricName": "accuracy",
          "metricValue": 0.8065580089675268,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.484Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj6h00gnlchsatebvw0j",
          "metricName": "accuracy",
          "metricValue": 0.8698792555018721,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.178Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljpq00jxlchsv0z032zv",
          "metricName": "accuracy",
          "metricValue": 0.8393071460116465,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.870Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk9b00n7lchsvd126lbv",
          "metricName": "accuracy",
          "metricValue": 0.7605081457206925,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.575Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkti00qhlchs28zt65et",
          "metricName": "overall_score",
          "metricValue": 6.030089035079597,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.302Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmg1010blchs04at1nsl",
          "metricName": "average_score",
          "metricValue": 62.56703026284399,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.410Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln03013llchsumpg2ph0",
          "metricName": "win_rate",
          "metricValue": 0.6866279512095215,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.132Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j6o03a1lcy4b6krgw1m",
          "metricName": "accuracy",
          "metricValue": 0.571173637854802,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.073Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jqc03dblcy4xax54lb0",
          "metricName": "success_rate",
          "metricValue": 0.4272792905024954,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.780Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13k9x03gllcy4uj6e6pmh",
          "metricName": "overall_score",
          "metricValue": 70.33893081830234,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.485Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ktf03jvlcy4l9m2oqt9",
          "metricName": "accuracy",
          "metricValue": 0.5068012345147324,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.188Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13ldg03n5lcy4xkvjfocm",
          "metricName": "truth_score",
          "metricValue": 0.7294500422407115,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.908Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lwy03qflcy40hftwl0x",
          "metricName": "accuracy",
          "metricValue": 0.4997959659133954,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.610Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtnl04f0lcy4tg80b0jh",
          "metricName": "percentile_rank",
          "metricValue": 10.99394228982624,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.170Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.034Z"
    },
    {
      "id": "cmewveluj0007lcp4m7jvm55d",
      "name": "gpt-5-low-fast",
      "displayName": "gpt-5-low-fast",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.0005,
      "pricingOutput": 0.001,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.576106472289966,
      "successRate": 0.8023676912560735,
      "passRate": 0.6950413579979491,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh6r003llchskwrpxb63",
          "metricName": "success_rate",
          "metricValue": 0.8023676912560735,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.595Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhmt006vlchsry86cdqq",
          "metricName": "pass_rate",
          "metricValue": 0.6950413579979491,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.173Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllq300vzlchsv78arnwh",
          "metricName": "success_rate",
          "metricValue": 0.9502466271153476,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.475Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllpr00vxlchsysu8dkn4",
          "metricName": "avg_response_time",
          "metricValue": 3.576106472289966,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.463Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli3p00a5lchs23sxkct1",
          "metricName": "pass_rate",
          "metricValue": 0.8481564745715242,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.781Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt7304bslcy4v9t66civ",
          "metricName": "elo_rating",
          "metricValue": 894.9588259855749,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.575Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlink00dflchs51z7eqwg",
          "metricName": "accuracy",
          "metricValue": 0.8606770108537327,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.496Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj6s00gplchsdk5zmktu",
          "metricName": "accuracy",
          "metricValue": 0.7911628356003134,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.188Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljq000jzlchsbhqtnvrs",
          "metricName": "accuracy",
          "metricValue": 0.7431037178044337,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.881Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk9m00n9lchsbgsutigl",
          "metricName": "accuracy",
          "metricValue": 0.6083825258299053,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.586Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlktu00qjlchsqwvqj5rq",
          "metricName": "overall_score",
          "metricValue": 9.630474717162866,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.314Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmgd010dlchs6s3of6xh",
          "metricName": "average_score",
          "metricValue": 74.36772267013019,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.422Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln0g013nlchst2hbqv0j",
          "metricName": "win_rate",
          "metricValue": 0.784555283469861,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.144Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j6z03a3lcy4340myprk",
          "metricName": "accuracy",
          "metricValue": 0.7521278050992836,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.083Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jqn03ddlcy4szi3n0nh",
          "metricName": "success_rate",
          "metricValue": 0.4228916613357251,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.791Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13ka803gnlcy419cpzlgp",
          "metricName": "overall_score",
          "metricValue": 67.44147447138623,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.497Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kts03jxlcy4zvl2g3mr",
          "metricName": "accuracy",
          "metricValue": 0.6818511789861581,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.200Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lds03n7lcy4oeefbgdm",
          "metricName": "truth_score",
          "metricValue": 0.5369791963702699,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.921Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lxa03qhlcy4d74spl52",
          "metricName": "accuracy",
          "metricValue": 0.430272881608433,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.622Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtnv04f2lcy47n19h3vo",
          "metricName": "percentile_rank",
          "metricValue": 32.19173095415253,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.179Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.043Z"
    },
    {
      "id": "cmewvelxl000jlcp4rie3jm1e",
      "name": "gpt-5-medium",
      "displayName": "gpt-5-medium",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.002,
      "pricingOutput": 0.01,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.663530884306563,
      "successRate": 0.9504370048469197,
      "passRate": 0.7558779292586997,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.7879999999999999,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh9y0049lchs590ch3uo",
          "metricName": "success_rate",
          "metricValue": 0.9504370048469197,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.711Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhqf007jlchs67k7jnpj",
          "metricName": "pass_rate",
          "metricValue": 0.7558779292586997,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.303Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllxx00xblchscr2kl7f7",
          "metricName": "success_rate",
          "metricValue": 0.8619804245819135,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.757Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllxl00x9lchso0ft8yf3",
          "metricName": "avg_response_time",
          "metricValue": 1.663530884306563,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.745Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli7l00atlchstbj8npuh",
          "metricName": "pass_rate",
          "metricValue": 0.6219851798530661,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.921Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtab04cglcy4xvqdrenj",
          "metricName": "elo_rating",
          "metricValue": 859.1076431092166,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.691Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlirk00e3lchs5tv8qaoa",
          "metricName": "accuracy",
          "metricValue": 0.7472873154345777,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.640Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljaq00hdlchs7jqof2r4",
          "metricName": "accuracy",
          "metricValue": 0.8168638577092647,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.331Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlju400knlchsfegcdhuq",
          "metricName": "accuracy",
          "metricValue": 0.8923344032307997,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.028Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkej00nxlchsh43lx0es",
          "metricName": "accuracy",
          "metricValue": 0.7685348251795194,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.763Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkxw00r7lchs0l5mbj76",
          "metricName": "overall_score",
          "metricValue": 9.889074558013242,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.460Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmks0111lchsql8coato",
          "metricName": "average_score",
          "metricValue": 64.17537898595513,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.580Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln4a014blchs76m45vz9",
          "metricName": "win_rate",
          "metricValue": 0.4654697385623998,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.282Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex0zw3u000ylcd42htbt58k",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.7879999999999999,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T16:07:01.194Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13jau03arlcy4np9nwgoo",
          "metricName": "accuracy",
          "metricValue": 0.7913685840623841,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.223Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jun03e1lcy4sd1z8jpz",
          "metricName": "success_rate",
          "metricValue": 0.4022595841403739,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.935Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13ke803hblcy48pqbou59",
          "metricName": "overall_score",
          "metricValue": 72.26258758413223,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.641Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kxq03kllcy4sno5ph2e",
          "metricName": "accuracy",
          "metricValue": 0.2363252167787909,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.343Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lhx03nvlcy4bixxuxfd",
          "metricName": "truth_score",
          "metricValue": 0.663122965222376,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.070Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m3b03r5lcy4bo2guugo",
          "metricName": "accuracy",
          "metricValue": 0.5099339350623863,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.840Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e14r0010lc1o13hex846",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.7879999999999999,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.891Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtrb04fqlcy4hulznkf8",
          "metricName": "percentile_rank",
          "metricValue": 54.01701527433102,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.304Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.153Z"
    },
    {
      "id": "cmewvelxt000klcp4btu8wzy1",
      "name": "gpt-5-medium-fast",
      "displayName": "gpt-5-medium-fast",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.001,
      "pricingOutput": 0.002,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.895892193076623,
      "successRate": 0.8849002546869421,
      "passRate": 0.9009628738370374,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlha7004blchs32uyttai",
          "metricName": "success_rate",
          "metricValue": 0.8849002546869421,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.719Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhqo007llchsyglwgvbj",
          "metricName": "pass_rate",
          "metricValue": 0.9009628738370374,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.312Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllyk00xflchscwspbxii",
          "metricName": "success_rate",
          "metricValue": 0.7241263099747678,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.781Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlly900xdlchsxahhhnt7",
          "metricName": "avg_response_time",
          "metricValue": 3.895892193076623,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.770Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli7z00avlchsbppd54g3",
          "metricName": "pass_rate",
          "metricValue": 0.8984524760200188,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.935Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtak04cilcy4c0l17yhi",
          "metricName": "elo_rating",
          "metricValue": 847.7308072022599,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.701Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlirw00e5lchs15o6vvw5",
          "metricName": "accuracy",
          "metricValue": 0.9312651962808125,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.652Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljb200hflchst3tvdfg3",
          "metricName": "accuracy",
          "metricValue": 0.531608794027921,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.342Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljuf00kplchsodcyn6m7",
          "metricName": "accuracy",
          "metricValue": 0.8710149493615869,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.040Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkew00nzlchs2ew9y6sm",
          "metricName": "accuracy",
          "metricValue": 0.871281246741122,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.776Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlky800r9lchscm8k1g3v",
          "metricName": "overall_score",
          "metricValue": 9.649885214431709,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.472Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlml30113lchs4xuu5258",
          "metricName": "average_score",
          "metricValue": 74.43913644055033,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.592Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln4n014dlchshoxy2smv",
          "metricName": "win_rate",
          "metricValue": 0.4179601304548738,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.295Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13jb603atlcy4j2s9o4mr",
          "metricName": "accuracy",
          "metricValue": 0.7526763725883324,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.234Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13juz03e3lcy4r5q5fref",
          "metricName": "success_rate",
          "metricValue": 0.5786193165952029,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.947Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kek03hdlcy4bcjogbg7",
          "metricName": "overall_score",
          "metricValue": 58.31518822148867,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.652Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ky403knlcy4vl486471",
          "metricName": "accuracy",
          "metricValue": 0.2246345186586554,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.356Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lia03nxlcy4bzbmk3m0",
          "metricName": "truth_score",
          "metricValue": 0.6534561697955921,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.082Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m3m03r7lcy49f6tlw8m",
          "metricName": "accuracy",
          "metricValue": 0.4899649392208464,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.851Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtrl04fslcy4ji3pno73",
          "metricName": "percentile_rank",
          "metricValue": 34.71303902733705,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.314Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.162Z"
    },
    {
      "id": "cmewvelv9000alcp4mneqalte",
      "name": "gpt-5-mini",
      "displayName": "gpt-5-mini",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.00015,
      "pricingOutput": 0.0006,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.830661255847053,
      "successRate": 0.8344444431720295,
      "passRate": 0.9329620028555021,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh7j003rlchsuh5azilb",
          "metricName": "success_rate",
          "metricValue": 0.8344444431720295,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.624Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhno0071lchstkx75v3n",
          "metricName": "pass_rate",
          "metricValue": 0.9329620028555021,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.204Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlls400wblchswpu26x3n",
          "metricName": "success_rate",
          "metricValue": 0.8071061997865148,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.548Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllrr00w9lchsc6n1hy08",
          "metricName": "avg_response_time",
          "metricValue": 2.830661255847053,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.536Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli4o00ablchsqs38a0fj",
          "metricName": "pass_rate",
          "metricValue": 0.542686630909933,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.816Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt7y04bylcy4ouvleiyf",
          "metricName": "elo_rating",
          "metricValue": 1109.820762107181,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.607Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlioj00dllchsmp4lbfe0",
          "metricName": "accuracy",
          "metricValue": 0.7689699704474667,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.531Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj7u00gvlchss00q7pc1",
          "metricName": "accuracy",
          "metricValue": 0.5239933846060569,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.227Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljqz00k5lchsocxzn9gb",
          "metricName": "accuracy",
          "metricValue": 0.7419876084192898,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.915Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkal00nflchsssi21kgg",
          "metricName": "accuracy",
          "metricValue": 0.9430107921270365,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.621Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkuu00qplchsmv8zjfat",
          "metricName": "overall_score",
          "metricValue": 9.913631668804136,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.350Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmhl010jlchs7y03jdov",
          "metricName": "average_score",
          "metricValue": 74.94748196102607,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.465Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln1d013tlchsjzyyykg0",
          "metricName": "win_rate",
          "metricValue": 0.6279359792305357,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.177Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j7y03a9lcy4pkz4v1ls",
          "metricName": "accuracy",
          "metricValue": 0.7728715178666599,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.118Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jrq03djlcy407nhdgp0",
          "metricName": "success_rate",
          "metricValue": 0.4104944224527899,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.830Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kb903gtlcy47qc47elq",
          "metricName": "overall_score",
          "metricValue": 68.21550739900103,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.533Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kur03k3lcy4fuxqdpfc",
          "metricName": "accuracy",
          "metricValue": 0.4370778209076642,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.235Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lep03ndlcy4z4on841v",
          "metricName": "truth_score",
          "metricValue": 0.7926253841733453,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.954Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m0803qnlcy4a3ry6w9u",
          "metricName": "accuracy",
          "metricValue": 0.5479166917349734,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.728Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtoq04f8lcy4evn5k3ag",
          "metricName": "percentile_rank",
          "metricValue": 19.15450755501274,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.211Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.070Z"
    },
    {
      "id": "cmewvelvi000blcp4jnzw0g2i",
      "name": "gpt-5-nano",
      "displayName": "gpt-5-nano",
      "provider": "OpenAI",
      "description": "Модель для программирования от OpenAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.000075,
      "pricingOutput": 0.0003,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.073920342783216,
      "successRate": 0.8568984854285155,
      "passRate": 0.7406004818466947,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh7u003tlchs9me4yz7q",
          "metricName": "success_rate",
          "metricValue": 0.8568984854285155,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.634Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlho30073lchsdicjuhuc",
          "metricName": "pass_rate",
          "metricValue": 0.7406004818466947,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.219Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllsq00wflchsw4jywfk8",
          "metricName": "success_rate",
          "metricValue": 0.9692251223768913,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.570Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllsg00wdlchstedyludi",
          "metricName": "avg_response_time",
          "metricValue": 2.073920342783216,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.560Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli4z00adlchsk0ga3jyp",
          "metricName": "pass_rate",
          "metricValue": 0.7250423707256298,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.827Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt8704c0lcy4177qnulw",
          "metricName": "elo_rating",
          "metricValue": 1075.162263930199,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.616Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvliov00dnlchsz4to8i5b",
          "metricName": "accuracy",
          "metricValue": 0.9464644784354017,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.543Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj8600gxlchs6goxn9jo",
          "metricName": "accuracy",
          "metricValue": 0.6341642993163477,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.238Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljrd00k7lchsg3it9cgp",
          "metricName": "accuracy",
          "metricValue": 0.8324668712127026,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.929Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkaw00nhlchsyqhlr0z9",
          "metricName": "accuracy",
          "metricValue": 0.7629981911528352,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.632Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkv500qrlchsyr612cch",
          "metricName": "overall_score",
          "metricValue": 7.798396847812762,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.361Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmhy010llchsxqt270mt",
          "metricName": "average_score",
          "metricValue": 70.36924088994219,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.478Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln1n013vlchs7g9dohlj",
          "metricName": "win_rate",
          "metricValue": 0.4920556495998128,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.188Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13j8803ablcy4dyygbsw4",
          "metricName": "accuracy",
          "metricValue": 0.5161955531284111,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.129Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13js203dllcy4d7fn8m2v",
          "metricName": "success_rate",
          "metricValue": 0.4630120453982226,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.842Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kbl03gvlcy4neilpe9m",
          "metricName": "overall_score",
          "metricValue": 73.88718833759206,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.545Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kv303k5lcy4dzmyz5mr",
          "metricName": "accuracy",
          "metricValue": 0.4246802537312315,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.247Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lf103nflcy4xswgunth",
          "metricName": "truth_score",
          "metricValue": 0.7420195265058426,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.965Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m0l03qplcy46gxl1iqt",
          "metricName": "accuracy",
          "metricValue": 0.3061336176816601,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.741Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtp204falcy4vevl7yuk",
          "metricName": "percentile_rank",
          "metricValue": 57.82918184035763,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.222Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.079Z"
    },
    {
      "id": "cmewvelz0000plcp4zct8hd8r",
      "name": "grok-3",
      "displayName": "grok-3",
      "provider": "xAI",
      "description": "Модель для программирования от xAI (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.002,
      "pricingOutput": 0.01,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"tool_use\",\"function_calling\",\"multi_step_reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.229512222817152,
      "successRate": 0.8680358074170289,
      "passRate": 0.6160444671908947,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.698,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlhbl004llchs1a2d9fi6",
          "metricName": "success_rate",
          "metricValue": 0.8680358074170289,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.770Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhrw007vlchsx9f9mqcl",
          "metricName": "pass_rate",
          "metricValue": 0.6160444671908947,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.356Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlm1y00xzlchs06edzjpq",
          "metricName": "success_rate",
          "metricValue": 0.9068009590516807,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.902Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlm1n00xxlchst18vaq6n",
          "metricName": "avg_response_time",
          "metricValue": 1.229512222817152,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.891Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli9r00b5lchs92m6ix5g",
          "metricName": "pass_rate",
          "metricValue": 0.7008710598824096,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:52.000Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtc604cslcy4mn01zfk2",
          "metricName": "elo_rating",
          "metricValue": 1340.475189541282,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.758Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlitl00eflchseh5vumgq",
          "metricName": "accuracy",
          "metricValue": 0.7505381881032246,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.713Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljcp00hplchsepsyc509",
          "metricName": "accuracy",
          "metricValue": 0.9200287180599375,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.402Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljw000kzlchs5vno4z3m",
          "metricName": "accuracy",
          "metricValue": 0.7204192006216569,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.096Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkgh00o9lchs1iy91jsq",
          "metricName": "accuracy",
          "metricValue": 0.7052212038083197,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.833Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkzt00rjlchsnquiuejl",
          "metricName": "overall_score",
          "metricValue": 7.431841272891163,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.529Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmms011dlchskraaxexo",
          "metricName": "average_score",
          "metricValue": 69.1606893502781,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.652Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln6b014nlchsz97ll7u0",
          "metricName": "win_rate",
          "metricValue": 0.6264718723942889,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.355Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjp9000olcj0xelu7tqq",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.698,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.269Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13jcx03b3lcy4e288houb",
          "metricName": "accuracy",
          "metricValue": 0.6345553334862626,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.297Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jwl03edlcy40k5qpwn8",
          "metricName": "success_rate",
          "metricValue": 0.476889123101832,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:52.005Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kg703hnlcy4n1ut01mm",
          "metricName": "overall_score",
          "metricValue": 56.67883049049043,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.711Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kzt03kxlcy48p1cqy3i",
          "metricName": "accuracy",
          "metricValue": 0.4193131195775603,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.417Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13ljy03o7lcy4cy7sbbsv",
          "metricName": "truth_score",
          "metricValue": 0.5252694217072984,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.142Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m5f03rhlcy4v2tvo0mp",
          "metricName": "accuracy",
          "metricValue": 0.4999613748102198,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.916Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e13j000qlc1oz5q8a2qb",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.698,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.848Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtt104g2lcy46gxetomt",
          "metricName": "percentile_rank",
          "metricValue": 15.26291158071836,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.366Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.204Z"
    },
    {
      "id": "cmewvelz9000qlcp4wwrn0c2k",
      "name": "grok-3-mini",
      "displayName": "grok-3-mini",
      "provider": "xAI",
      "description": "Модель для программирования от xAI",
      "contextWindow": 128000,
      "pricingInput": 0.0008,
      "pricingOutput": 0.004,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.344529643526987,
      "successRate": 0.7022895578186706,
      "passRate": 0.715030888591267,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.6709999999999999,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlhbv004nlchsj2kt7yfr",
          "metricName": "success_rate",
          "metricValue": 0.7022895578186706,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.779Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhs6007xlchs5pk163q4",
          "metricName": "pass_rate",
          "metricValue": 0.715030888591267,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.366Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlm2p00y3lchs9s8930b7",
          "metricName": "success_rate",
          "metricValue": 0.9863071595642585,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.929Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlm2a00y1lchsoi6t62b3",
          "metricName": "avg_response_time",
          "metricValue": 3.344529643526987,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.914Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlia300b7lchs4qqy8y20",
          "metricName": "pass_rate",
          "metricValue": 0.5053746088859524,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:52.012Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtci04culcy4bshe7wby",
          "metricName": "elo_rating",
          "metricValue": 915.8016704906017,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.771Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlitw00ehlchsxy22lkzz",
          "metricName": "accuracy",
          "metricValue": 0.9671902835697781,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.724Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljd100hrlchs3beknb4v",
          "metricName": "accuracy",
          "metricValue": 0.8221197042221654,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.414Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljwb00l1lchsumv9ocrj",
          "metricName": "accuracy",
          "metricValue": 0.7499380826613689,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.107Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkgt00oblchs2nonclfd",
          "metricName": "accuracy",
          "metricValue": 0.7064090767017895,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.845Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvll0500rllchsbmb6i9mq",
          "metricName": "overall_score",
          "metricValue": 9.830771191019426,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.541Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmn3011flchs1wd64qkr",
          "metricName": "average_score",
          "metricValue": 66.74248698063838,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.663Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln6m014plchsf9ib6ssc",
          "metricName": "win_rate",
          "metricValue": 0.7626582854255013,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.366Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex0zw450010lcd4xine49sw",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.6709999999999999,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T16:07:01.205Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13jd803b5lcy4ut76sm46",
          "metricName": "accuracy",
          "metricValue": 0.5494576092970378,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.309Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jww03eflcy4hkiyvjxl",
          "metricName": "success_rate",
          "metricValue": 0.3591672531510799,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:52.017Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kgj03hplcy4m7fhujpp",
          "metricName": "overall_score",
          "metricValue": 69.56104536721409,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.723Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13l0503kzlcy4lw967lqb",
          "metricName": "accuracy",
          "metricValue": 0.2476480700153892,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.429Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lk903o9lcy4a09lljl3",
          "metricName": "truth_score",
          "metricValue": 0.5890737850239554,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.153Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m5r03rjlcy4exayebwe",
          "metricName": "accuracy",
          "metricValue": 0.5401049336484942,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.928Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e14z0012lc1oaeo92g98",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.6709999999999999,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.900Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtta04g4lcy4d9x732ju",
          "metricName": "percentile_rank",
          "metricValue": 77.55725074221606,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.375Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.213Z"
    },
    {
      "id": "cmewvelw8000elcp45fgsmnl4",
      "name": "grok-4",
      "displayName": "grok-4",
      "provider": "xAI",
      "description": "Модель для программирования от xAI",
      "contextWindow": 128000,
      "pricingInput": 0.002,
      "pricingOutput": 0.01,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "coding",
      "capabilities": "[\"tool_use\",\"function_calling\",\"multi_step_reasoning\",\"code_generation\",\"analysis\",\"real_time_knowledge\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.522287724843608,
      "successRate": 0.8777378816853825,
      "passRate": 0.730347169905025,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.713,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh8l003zlchslijq154l",
          "metricName": "success_rate",
          "metricValue": 0.8777378816853825,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.661Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhow0079lchsj1n62i1p",
          "metricName": "pass_rate",
          "metricValue": 0.730347169905025,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.249Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlluq00wrlchsmfpqmibi",
          "metricName": "success_rate",
          "metricValue": 0.9830830580601548,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.642Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllue00wplchs1o4cdq3f",
          "metricName": "avg_response_time",
          "metricValue": 1.522287724843608,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.631Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli5v00ajlchs8tcutxey",
          "metricName": "pass_rate",
          "metricValue": 0.6252347721591727,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.859Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt9204c6lcy4z75gscqu",
          "metricName": "elo_rating",
          "metricValue": 901.6239633703706,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.647Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlipw00dtlchsu4xq78ev",
          "metricName": "accuracy",
          "metricValue": 0.9897051859129171,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.580Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj9300h3lchs1v4txuxl",
          "metricName": "accuracy",
          "metricValue": 0.5594983812797424,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.272Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljsf00kdlchsdumti49i",
          "metricName": "accuracy",
          "metricValue": 0.8190687841805085,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.968Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkby00nnlchss8tnpmpm",
          "metricName": "accuracy",
          "metricValue": 0.8998609251033891,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.670Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkw500qxlchsal035q90",
          "metricName": "overall_score",
          "metricValue": 6.7744797052939,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.397Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmj2010rlchs3lp7c733",
          "metricName": "average_score",
          "metricValue": 76.66303877355472,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.518Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln2k0141lchsv2qs3rm8",
          "metricName": "win_rate",
          "metricValue": 0.4660123943506331,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.220Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjp0000mlcj08suvulrq",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.713,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.261Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j9603ahlcy4aohci905",
          "metricName": "accuracy",
          "metricValue": 0.467042247681336,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.162Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jt103drlcy4sf95i215",
          "metricName": "success_rate",
          "metricValue": 0.3884755156820761,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.877Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kcl03h1lcy4hcd3umge",
          "metricName": "overall_score",
          "metricValue": 58.48513647385616,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.581Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kw303kblcy45706vo6e",
          "metricName": "accuracy",
          "metricValue": 0.2987909397068007,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.284Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lg303nllcy49ifpph96",
          "metricName": "truth_score",
          "metricValue": 0.7319913377243272,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.003Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m1o03qvlcy4lwrmrac6",
          "metricName": "accuracy",
          "metricValue": 0.5169951949518075,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.780Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e13b000olc1onw56sr3c",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.713,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.840Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtpw04fglcy4kojknuyn",
          "metricName": "percentile_rank",
          "metricValue": 73.12261135759925,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.252Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.105Z"
    },
    {
      "id": "cmewvelws000glcp4gzyfm9ad",
      "name": "grok-code-fast-1",
      "displayName": "grok-code-fast-1",
      "provider": "xAI",
      "description": "Модель для программирования от xAI",
      "contextWindow": 128000,
      "pricingInput": 0.002,
      "pricingOutput": 0.01,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 3.107083174509613,
      "successRate": 0.9770539787324054,
      "passRate": 0.8234957097813825,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.6679999999999999,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh960043lchsyl1xuua4",
          "metricName": "success_rate",
          "metricValue": 0.9770539787324054,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.682Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhpi007dlchsk15wqmmp",
          "metricName": "pass_rate",
          "metricValue": 0.8234957097813825,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.271Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllw000wzlchs2tir8l3c",
          "metricName": "success_rate",
          "metricValue": 0.8731295179842243,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.688Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllvo00wxlchsh1jzg0s2",
          "metricName": "avg_response_time",
          "metricValue": 3.107083174509613,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.676Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli6h00anlchszl8xk6zu",
          "metricName": "pass_rate",
          "metricValue": 0.6589922808444706,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.881Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt9k04calcy4p65xasd7",
          "metricName": "elo_rating",
          "metricValue": 1764.175989835241,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.664Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvliqk00dxlchsrd16bl1e",
          "metricName": "accuracy",
          "metricValue": 0.7543340519201108,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.604Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj9s00h7lchsx9tlteqo",
          "metricName": "accuracy",
          "metricValue": 0.8747092211338436,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.296Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljt500khlchs5a4z4oqs",
          "metricName": "accuracy",
          "metricValue": 0.9255480922459357,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.993Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkde00nrlchs4yvens5d",
          "metricName": "accuracy",
          "metricValue": 0.6569658961519099,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.722Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkwu00r1lchsel5y32j1",
          "metricName": "overall_score",
          "metricValue": 8.57861067487203,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.422Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmjs010vlchs611kdkqo",
          "metricName": "average_score",
          "metricValue": 65.02429664248622,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.544Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln3a0145lchsb567cm9a",
          "metricName": "win_rate",
          "metricValue": 0.4809033075707997,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.246Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex0zw4f0012lcd4va0el95j",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.6679999999999999,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T16:07:01.216Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j9v03allcy46zai6c5o",
          "metricName": "accuracy",
          "metricValue": 0.424865050986508,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.187Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jto03dvlcy4yotuej8r",
          "metricName": "success_rate",
          "metricValue": 0.5200506640468795,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.901Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kd903h5lcy4uoqegai2",
          "metricName": "overall_score",
          "metricValue": 52.59554112989864,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.606Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kwr03kflcy4be8tqrmy",
          "metricName": "accuracy",
          "metricValue": 0.4417738536104329,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.307Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lgx03nplcy4ducv5280",
          "metricName": "truth_score",
          "metricValue": 0.5001143522127328,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.033Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m2b03qzlcy4dcmd7tl9",
          "metricName": "accuracy",
          "metricValue": 0.5281810050295377,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.803Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e1580014lc1oaxlq5c2r",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.6679999999999999,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.908Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtqg04fklcy4fjqhuyq9",
          "metricName": "percentile_rank",
          "metricValue": 12.72501848428277,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.273Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.124Z"
    },
    {
      "id": "cmewvelwh000flcp4j5c6qgo3",
      "name": "kimi-k2-instruct",
      "displayName": "kimi-k2-instruct",
      "provider": "Kimi",
      "description": "Модель для программирования от Kimi (с reasoning)",
      "contextWindow": 128000,
      "pricingInput": 0.0005,
      "pricingOutput": 0.001,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": false,
      "category": "coding",
      "capabilities": "[\"code_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.36389882748721,
      "successRate": 0.7861121842516821,
      "passRate": 0.6455713182111907,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.652,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh8w0041lchsjt8y1w1r",
          "metricName": "success_rate",
          "metricValue": 0.7861121842516821,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.672Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhp8007blchsrvqhqdx4",
          "metricName": "pass_rate",
          "metricValue": 0.6455713182111907,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.260Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllvd00wvlchss0tf10d8",
          "metricName": "success_rate",
          "metricValue": 0.8837631110962612,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.666Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllv100wtlchsgb03eq7o",
          "metricName": "avg_response_time",
          "metricValue": 1.36389882748721,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.654Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli6500allchsy9at8cnw",
          "metricName": "pass_rate",
          "metricValue": 0.7907018484515334,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.870Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt9b04c8lcy4ok92hkt4",
          "metricName": "elo_rating",
          "metricValue": 1477.33041187515,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.655Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvliq800dvlchsx61jw6su",
          "metricName": "accuracy",
          "metricValue": 0.7907483445763805,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.593Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj9g00h5lchsxp3ftdsa",
          "metricName": "accuracy",
          "metricValue": 0.7731956873396912,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.284Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljss00kflchsxqlrszp5",
          "metricName": "accuracy",
          "metricValue": 0.8755116336757349,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.980Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkce00nplchs1f9uu175",
          "metricName": "accuracy",
          "metricValue": 0.9168549924436321,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.686Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkwh00qzlchspmlduvqk",
          "metricName": "overall_score",
          "metricValue": 6.501406105899143,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.409Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmjf010tlchsy21zqkdz",
          "metricName": "average_score",
          "metricValue": 79.30067170762783,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.531Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln2x0143lchsxpr564fa",
          "metricName": "win_rate",
          "metricValue": 0.6334475334408172,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.234Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjpi000qlcj0itsk8m78",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.652,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.279Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j9h03ajlcy47ts85bb7",
          "metricName": "accuracy",
          "metricValue": 0.4600930087967379,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.174Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jtd03dtlcy4qu7569o3",
          "metricName": "success_rate",
          "metricValue": 0.4674966537645602,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.890Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kcx03h3lcy4svjngro5",
          "metricName": "overall_score",
          "metricValue": 61.30897435969675,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.594Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kwg03kdlcy40jjx3cf7",
          "metricName": "accuracy",
          "metricValue": 0.4796070335080576,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.296Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lgh03nnlcy4yirv683n",
          "metricName": "truth_score",
          "metricValue": 0.773382579659856,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.017Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m1z03qxlcy4ah58is9z",
          "metricName": "accuracy",
          "metricValue": 0.4057721500464527,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.792Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e13u000slc1o5mtuzzzk",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.652,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.858Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gtq604filcy4op0o5vj8",
          "metricName": "percentile_rank",
          "metricValue": 24.51895485293302,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.263Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.113Z"
    },
    {
      "id": "cmewvelus0008lcp44nxgn8i4",
      "name": "o3",
      "displayName": "o3",
      "provider": "OpenAI",
      "description": "Универсальная модель ИИ от OpenAI (с reasoning)",
      "contextWindow": 200000,
      "pricingInput": 0.01,
      "pricingOutput": 0.03,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": true,
      "isAgent": true,
      "category": "reasoning",
      "capabilities": "[\"advanced_reasoning\",\"multi_step_planning\",\"code_generation\",\"analysis\",\"self_reflection\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 1.128874326562864,
      "successRate": 0.9161002449686313,
      "passRate": 0.700309475502029,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.813,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlh70003nlchsdtejskgp",
          "metricName": "success_rate",
          "metricValue": 0.9161002449686313,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.605Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhn5006xlchs9aweribm",
          "metricName": "pass_rate",
          "metricValue": 0.700309475502029,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.185Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvllqp00w3lchs8ywgqojx",
          "metricName": "success_rate",
          "metricValue": 0.9589815879559294,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.498Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvllqe00w1lchsfahiax2b",
          "metricName": "avg_response_time",
          "metricValue": 1.128874326562864,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.487Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli4000a7lchsyz5wefmk",
          "metricName": "pass_rate",
          "metricValue": 0.8411326295981878,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.793Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gt7d04bulcy4j4c67p45",
          "metricName": "elo_rating",
          "metricValue": 1547.531241546993,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.586Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlinv00dhlchs7wcc3aw1",
          "metricName": "accuracy",
          "metricValue": 0.936916494698365,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.508Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvlj7600grlchsus88czmq",
          "metricName": "accuracy",
          "metricValue": 0.7023430795931701,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.202Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljqd00k1lchs1vxtna5q",
          "metricName": "accuracy",
          "metricValue": 0.6967077001094373,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:53.893Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlk9y00nblchsvfutds0n",
          "metricName": "accuracy",
          "metricValue": 0.7789793252234308,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.598Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlku700qllchsdy6g1bxf",
          "metricName": "overall_score",
          "metricValue": 9.021858282082263,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.327Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmgo010flchsf83n62gv",
          "metricName": "average_score",
          "metricValue": 65.50092795990446,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.432Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln0r013plchss3nqsmba",
          "metricName": "win_rate",
          "metricValue": 0.7748819261109682,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.155Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjmr0004lcj0e5npedk5",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.813,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.179Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13j7b03a5lcy4f61dzkxe",
          "metricName": "accuracy",
          "metricValue": 0.5441149628739379,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.095Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jr003dflcy4a2zj0u9y",
          "metricName": "success_rate",
          "metricValue": 0.4162025184352489,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.804Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kal03gplcy4p381j4ef",
          "metricName": "overall_score",
          "metricValue": 79.12207223629606,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.509Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13ku303jzlcy4nhuk7d71",
          "metricName": "accuracy",
          "metricValue": 0.6715058547692787,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.211Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13le403n9lcy4yw9cobi9",
          "metricName": "truth_score",
          "metricValue": 0.6889655566033188,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:53.932Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13lxn03qjlcy43ixdsqgv",
          "metricName": "accuracy",
          "metricValue": 0.4506585407177268,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.635Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e1180006lc1oqxcvdys6",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.813,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.765Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gto504f4lcy4ej1s6f7k",
          "metricName": "percentile_rank",
          "metricValue": 14.38041923921184,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.189Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.053Z"
    },
    {
      "id": "cmewvelzi000rlcp401eaqc40",
      "name": "o3-pro",
      "displayName": "o3-pro",
      "provider": "OpenAI",
      "description": "Универсальная модель ИИ от OpenAI",
      "contextWindow": 200000,
      "pricingInput": 0.03,
      "pricingOutput": 0.06,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": true,
      "category": "reasoning",
      "capabilities": "[\"tool_use\",\"function_calling\",\"multi_step_reasoning\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.36495087588801,
      "successRate": 0.8933010909997168,
      "passRate": 0.8194677235133547,
      "totalRatings": 0,
      "totalBenchmarks": 22,
      "aiderBenchmark": 0.8490000000000001,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlhc3004plchs1xwj8s5j",
          "metricName": "success_rate",
          "metricValue": 0.8933010909997168,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.788Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhsg007zlchs00uaawe4",
          "metricName": "pass_rate",
          "metricValue": 0.8194677235133547,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.376Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlm3b00y7lchsw8obbmf5",
          "metricName": "success_rate",
          "metricValue": 0.956272562613941,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.952Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlm3100y5lchsqi2smw83",
          "metricName": "avg_response_time",
          "metricValue": 2.36495087588801,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.941Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvliai00b9lchs6vfr50j4",
          "metricName": "pass_rate",
          "metricValue": 0.8621022392415402,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:52.026Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtcs04cwlcy4h7i8tfwu",
          "metricName": "elo_rating",
          "metricValue": 1236.797207764206,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.781Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvliu800ejlchsze4erbjd",
          "metricName": "accuracy",
          "metricValue": 0.9511220968677762,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.736Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljde00htlchs6kdt51sa",
          "metricName": "accuracy",
          "metricValue": 0.6831558260502099,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.426Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljwn00l3lchse6dh0ed2",
          "metricName": "accuracy",
          "metricValue": 0.9181163029928563,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.119Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkh400odlchsyw31qntq",
          "metricName": "accuracy",
          "metricValue": 0.8234292125762118,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.856Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvll0i00rnlchstvbihonm",
          "metricName": "overall_score",
          "metricValue": 9.469602315139195,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.554Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmng011hlchskz029rk9",
          "metricName": "average_score",
          "metricValue": 62.57800709723944,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.677Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln6y014rlchslharkuux",
          "metricName": "win_rate",
          "metricValue": 0.6293683765397184,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.378Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewxzjmh0002lcj0s7qji2qf",
          "metricName": "aider_benchmark_score",
          "metricValue": 0.8490000000000001,
          "benchmarkType": "aider",
          "createdAt": "2025-08-29T14:42:46.169Z",
          "source": {
            "id": "cmewxzjm50000lcj0zes4l2xu",
            "name": "aider_official",
            "displayName": "Aider Official Leaderboard",
            "description": "Официальные результаты бенчмарка Aider с сайта https://aider.chat/docs/leaderboards/",
            "category": "independent"
          }
        },
        {
          "id": "cmex13jdm03b7lcy4rbnkr4pq",
          "metricName": "accuracy",
          "metricValue": 0.6259551292418831,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.322Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jx703ehlcy43w4tghlc",
          "metricName": "success_rate",
          "metricValue": 0.4207581803969879,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:52.027Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kgu03hrlcy437quqn47",
          "metricName": "overall_score",
          "metricValue": 64.3711003650773,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.735Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13l0h03l1lcy4earewjxc",
          "metricName": "accuracy",
          "metricValue": 0.2300964636964981,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.441Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13lkk03oblcy4mgopsxdh",
          "metricName": "truth_score",
          "metricValue": 0.813265152375526,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.165Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m6303rllcy4xdieon7l",
          "metricName": "accuracy",
          "metricValue": 0.5747661667736347,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.939Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1e10x0004lc1ovpdotqpg",
          "metricName": "hellaswag_accuracy",
          "metricValue": 0.8490000000000001,
          "benchmarkType": "hellaswag",
          "createdAt": "2025-08-29T16:18:00.753Z",
          "source": {
            "id": "cmex1e10d0000lc1oa7hwievh",
            "name": "hellaswag_official",
            "displayName": "HellaSwag Official Benchmark",
            "description": "Официальные результаты бенчмарка HellaSwag - тест на здравый смысл и рассуждения",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex1gttl04g6lcy4tbo66jkv",
          "metricName": "percentile_rank",
          "metricValue": 43.5595841303223,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.385Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.223Z"
    },
    {
      "id": "cmewvelys000olcp4ujww7znw",
      "name": "o4-mini",
      "displayName": "o4-mini",
      "provider": "OpenAI",
      "description": "Универсальная модель ИИ от OpenAI",
      "contextWindow": 128000,
      "pricingInput": 0.00015,
      "pricingOutput": 0.0006,
      "isFree": false,
      "isRecommended": false,
      "isAvailableInCursor": true,
      "isReasoning": false,
      "isAgent": false,
      "category": "reasoning",
      "capabilities": "[\"text_generation\",\"analysis\",\"problem_solving\"]",
      "avgRating": null,
      "avgSpeedRating": null,
      "avgQualityRating": null,
      "avgCostRating": null,
      "avgResponseTime": 2.069591224385744,
      "successRate": 0.7246624778458443,
      "passRate": 0.8048696724692339,
      "totalRatings": 0,
      "totalBenchmarks": 20,
      "aiderBenchmark": null,
      "costValueScore": null,
      "userRatings": [],
      "benchmarkResults": [
        {
          "id": "cmewvlhbc004jlchs8rztrntm",
          "metricName": "success_rate",
          "metricValue": 0.7246624778458443,
          "benchmarkType": "coding_tasks",
          "createdAt": "2025-08-29T13:35:50.760Z",
          "source": {
            "id": "cmewtbsdn0004lcikw04d3s3o",
            "name": "aider",
            "displayName": "Aider Benchmarks",
            "description": "Real-world coding tasks with human evaluation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlhro007tlchseoj8n9bo",
          "metricName": "pass_rate",
          "metricValue": 0.8048696724692339,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.348Z",
          "source": {
            "id": "cmewtbse30005lcik0dutbm4b",
            "name": "humaneval",
            "displayName": "HumanEval",
            "description": "Programming problems for code generation",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlm1a00xvlchsfx4ayizm",
          "metricName": "success_rate",
          "metricValue": 0.9808529181784367,
          "benchmarkType": "code_quality",
          "createdAt": "2025-08-29T13:35:56.878Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvlm0y00xtlchsvgdwxsxg",
          "metricName": "avg_response_time",
          "metricValue": 2.069591224385744,
          "benchmarkType": "response_time",
          "createdAt": "2025-08-29T13:35:56.866Z",
          "source": {
            "id": "cmewtbsep0006lcikbvxtwq3q",
            "name": "cursor_internal",
            "displayName": "Cursor Internal Tests",
            "description": "Internal Cursor performance tests",
            "category": "mixed"
          }
        },
        {
          "id": "cmewvli9e00b3lchsicypzla5",
          "metricName": "pass_rate",
          "metricValue": 0.8213589397608643,
          "benchmarkType": "code_generation",
          "createdAt": "2025-08-29T13:35:51.987Z",
          "source": {
            "id": "cmewtoqfo0002lcros5zyjnot",
            "name": "mbpp",
            "displayName": "MBPP",
            "description": "Mostly Basic Programming Problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex1gtbu04cqlcy4nsu5kkq2",
          "metricName": "elo_rating",
          "metricValue": 1584.798614359294,
          "benchmarkType": "competitive_programming",
          "createdAt": "2025-08-29T16:20:10.747Z",
          "source": {
            "id": "cmewtoqfz0003lcroauz7t5ol",
            "name": "codeforces",
            "displayName": "CodeForces",
            "description": "Competitive programming contests and AI benchmark for competition-level code generation with CodeElo ratings",
            "category": "coding"
          }
        },
        {
          "id": "cmewvlita00edlchsgurpz9pp",
          "metricName": "accuracy",
          "metricValue": 0.9161820715439315,
          "benchmarkType": "knowledge",
          "createdAt": "2025-08-29T13:35:52.702Z",
          "source": {
            "id": "cmewtoqg90004lcroi9ddun9l",
            "name": "mmlu",
            "displayName": "MMLU",
            "description": "Massive Multitask Language Understanding",
            "category": "knowledge"
          }
        },
        {
          "id": "cmewvljcc00hnlchstogzlwiu",
          "metricName": "accuracy",
          "metricValue": 0.5811535618992252,
          "benchmarkType": "reasoning",
          "createdAt": "2025-08-29T13:35:53.389Z",
          "source": {
            "id": "cmewtoqgj0005lcro1xrmg4n0",
            "name": "arc",
            "displayName": "ARC",
            "description": "AI2 Reasoning Challenge",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvljvo00kxlchssd8gwhy9",
          "metricName": "accuracy",
          "metricValue": 0.7326022496447369,
          "benchmarkType": "commonsense",
          "createdAt": "2025-08-29T13:35:54.084Z",
          "source": {
            "id": "cmewtoqgs0006lcro1o7dz1vr",
            "name": "hellaswag",
            "displayName": "HellaSwag",
            "description": "Commonsense reasoning benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmewvlkg500o7lchskcxl9ay9",
          "metricName": "accuracy",
          "metricValue": 0.6119254644477526,
          "benchmarkType": "math",
          "createdAt": "2025-08-29T13:35:54.821Z",
          "source": {
            "id": "cmewtoqh90008lcro2il13nt9",
            "name": "gsm8k",
            "displayName": "GSM8K",
            "description": "Grade School Math 8K problems",
            "category": "math"
          }
        },
        {
          "id": "cmewvlkzi00rhlchszurxarie",
          "metricName": "overall_score",
          "metricValue": 8.207573616402716,
          "benchmarkType": "conversation",
          "createdAt": "2025-08-29T13:35:55.518Z",
          "source": {
            "id": "cmewtoqio000elcroe3t67r7a",
            "name": "mt_bench",
            "displayName": "MT-Bench",
            "description": "Multi-turn conversation benchmark",
            "category": "chat"
          }
        },
        {
          "id": "cmewvlmmf011blchsnad6e6lu",
          "metricName": "average_score",
          "metricValue": 71.66293546733094,
          "benchmarkType": "comprehensive",
          "createdAt": "2025-08-29T13:35:57.640Z",
          "source": {
            "id": "cmewtoqjv000jlcroaxqwr2mb",
            "name": "open_llm_leaderboard",
            "displayName": "Open LLM Leaderboard",
            "description": "Comprehensive LLM evaluation",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmewvln5y014llchszfnt82vi",
          "metricName": "win_rate",
          "metricValue": 0.4074623057153303,
          "benchmarkType": "chat_preference",
          "createdAt": "2025-08-29T13:35:58.343Z",
          "source": {
            "id": "cmewtoqk4000klcroyi7gwimz",
            "name": "chatbot_leaderboard",
            "displayName": "Chatbot Leaderboard",
            "description": "Chat model comparison",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13jcj03b1lcy4sgg6ipy3",
          "metricName": "accuracy",
          "metricValue": 0.7976318446403146,
          "benchmarkType": "multimodal_understanding",
          "createdAt": "2025-08-29T16:09:51.284Z",
          "source": {
            "id": "cmex13dd402aylcy43von60v9",
            "name": "mmmu",
            "displayName": "MMMU",
            "description": "Massive Multi-discipline Multimodal Understanding benchmark for evaluating multimodal models",
            "category": "multimodal"
          }
        },
        {
          "id": "cmex13jwa03eblcy4oglgox2j",
          "metricName": "success_rate",
          "metricValue": 0.3701126908951116,
          "benchmarkType": "code_repair",
          "createdAt": "2025-08-29T16:09:51.994Z",
          "source": {
            "id": "cmex13ddm02azlcy4xdhn6pcr",
            "name": "swe_bench",
            "displayName": "SWE-Bench",
            "description": "Benchmark for evaluating real-world software engineering problems",
            "category": "coding"
          }
        },
        {
          "id": "cmex13kfv03hllcy4l896ovz4",
          "metricName": "overall_score",
          "metricValue": 66.46433260424763,
          "benchmarkType": "comprehensive_evaluation",
          "createdAt": "2025-08-29T16:09:52.699Z",
          "source": {
            "id": "cmex13ddv02b0lcy43vjxcvuk",
            "name": "helm",
            "displayName": "HELM",
            "description": "Holistic Evaluation of Language Models (broad coverage of core scenarios and metrics)",
            "category": "comprehensive"
          }
        },
        {
          "id": "cmex13kzg03kvlcy4sduvwrl3",
          "metricName": "accuracy",
          "metricValue": 0.3456234803029604,
          "benchmarkType": "hard_reasoning",
          "createdAt": "2025-08-29T16:09:53.405Z",
          "source": {
            "id": "cmex13de502b1lcy4ccczkw07",
            "name": "bbh",
            "displayName": "BIG-Bench Hard",
            "description": "A subset of the most challenging tasks from the BIG-Bench benchmark",
            "category": "reasoning"
          }
        },
        {
          "id": "cmex13ljm03o5lcy45je34vd1",
          "metricName": "truth_score",
          "metricValue": 0.7450376892612052,
          "benchmarkType": "truthfulness",
          "createdAt": "2025-08-29T16:09:54.131Z",
          "source": {
            "id": "cmex13dee02b2lcy47w41maws",
            "name": "truthfulqa",
            "displayName": "TruthfulQA",
            "description": "Benchmark to measure whether a language model is truthful in generating answers to questions",
            "category": "safety"
          }
        },
        {
          "id": "cmex13m5203rflcy4xz77d67c",
          "metricName": "accuracy",
          "metricValue": 0.3351714186492589,
          "benchmarkType": "scientific_reasoning",
          "createdAt": "2025-08-29T16:09:54.903Z",
          "source": {
            "id": "cmex13den02b3lcy4mzxlbdwr",
            "name": "gpqa_diamond",
            "displayName": "GPQA Diamond",
            "description": "More challenging subset of the GPQA benchmark for advanced scientific reasoning",
            "category": "science"
          }
        },
        {
          "id": "cmex1gtsr04g0lcy4kw1qpwo9",
          "metricName": "percentile_rank",
          "metricValue": 49.52888380982296,
          "benchmarkType": "human_comparable_rating",
          "createdAt": "2025-08-29T16:20:11.356Z",
          "source": {
            "id": "cmex1grgb03z5lcy45rvvo957",
            "name": "codeelo",
            "displayName": "CodeElo",
            "description": "Human-comparable Elo ratings for AI models in competitive programming",
            "category": "coding"
          }
        }
      ],
      "createdAt": "2025-08-29T13:30:30.196Z"
    }
  ],
  "exportDate": "2025-09-10T22:10:46.774Z",
  "totalModels": 59,
  "totalRatings": 0,
  "totalBenchmarks": 1228
}